{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwQC2wYzXf8Jmrq4gZY3zv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adysinghh/T--T-GAN/blob/main/T_gan_good.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EOGOmtx1JwBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5100115-5549-46e1-a2f0-934a8b615bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset generated and saved in T_dataset\n",
            "Epoch [1/1200] Batch [0/8] Loss D: 1.5950, Loss G: 0.7502\n",
            "Epoch [2/1200] Batch [0/8] Loss D: 0.3322, Loss G: 2.0094\n",
            "Epoch [3/1200] Batch [0/8] Loss D: 0.1804, Loss G: 2.6188\n",
            "Epoch [4/1200] Batch [0/8] Loss D: 0.1029, Loss G: 3.1705\n",
            "Epoch [5/1200] Batch [0/8] Loss D: 0.0782, Loss G: 3.3777\n",
            "Epoch [6/1200] Batch [0/8] Loss D: 0.0759, Loss G: 3.6046\n",
            "Epoch [7/1200] Batch [0/8] Loss D: 0.0750, Loss G: 3.6795\n",
            "Epoch [8/1200] Batch [0/8] Loss D: 0.0912, Loss G: 3.7297\n",
            "Epoch [9/1200] Batch [0/8] Loss D: 1.9622, Loss G: 6.4575\n",
            "Epoch [10/1200] Batch [0/8] Loss D: 0.7902, Loss G: 1.9705\n",
            "Epoch [11/1200] Batch [0/8] Loss D: 0.4366, Loss G: 2.0440\n",
            "Epoch [12/1200] Batch [0/8] Loss D: 0.2358, Loss G: 2.9733\n",
            "Epoch [13/1200] Batch [0/8] Loss D: 0.1594, Loss G: 3.1679\n",
            "Epoch [14/1200] Batch [0/8] Loss D: 0.0855, Loss G: 3.7800\n",
            "Epoch [15/1200] Batch [0/8] Loss D: 0.0697, Loss G: 4.1705\n",
            "Epoch [16/1200] Batch [0/8] Loss D: 0.0312, Loss G: 4.5057\n",
            "Epoch [17/1200] Batch [0/8] Loss D: 0.0257, Loss G: 4.6277\n",
            "Epoch [18/1200] Batch [0/8] Loss D: 0.0190, Loss G: 4.8216\n",
            "Epoch [19/1200] Batch [0/8] Loss D: 0.0344, Loss G: 4.6611\n",
            "Epoch [20/1200] Batch [0/8] Loss D: 0.0927, Loss G: 4.3907\n",
            "Epoch [21/1200] Batch [0/8] Loss D: 0.6379, Loss G: 2.3999\n",
            "Epoch [22/1200] Batch [0/8] Loss D: 0.1448, Loss G: 3.3921\n",
            "Epoch [23/1200] Batch [0/8] Loss D: 0.0792, Loss G: 4.0839\n",
            "Epoch [24/1200] Batch [0/8] Loss D: 0.0774, Loss G: 4.1269\n",
            "Epoch [25/1200] Batch [0/8] Loss D: 0.0536, Loss G: 4.3754\n",
            "Epoch [26/1200] Batch [0/8] Loss D: 0.0357, Loss G: 4.6743\n",
            "Epoch [27/1200] Batch [0/8] Loss D: 0.0436, Loss G: 4.6198\n",
            "Epoch [28/1200] Batch [0/8] Loss D: 0.0389, Loss G: 4.4799\n",
            "Epoch [29/1200] Batch [0/8] Loss D: 0.0303, Loss G: 4.9070\n",
            "Epoch [30/1200] Batch [0/8] Loss D: 0.0242, Loss G: 4.9372\n",
            "Epoch [31/1200] Batch [0/8] Loss D: 0.0403, Loss G: 4.6826\n",
            "Epoch [32/1200] Batch [0/8] Loss D: 0.0521, Loss G: 4.5085\n",
            "Epoch [33/1200] Batch [0/8] Loss D: 0.0355, Loss G: 4.6590\n",
            "Epoch [34/1200] Batch [0/8] Loss D: 0.0442, Loss G: 4.6552\n",
            "Epoch [35/1200] Batch [0/8] Loss D: 0.0494, Loss G: 4.6021\n",
            "Epoch [36/1200] Batch [0/8] Loss D: 0.0427, Loss G: 4.7134\n",
            "Epoch [37/1200] Batch [0/8] Loss D: 0.0284, Loss G: 5.1550\n",
            "Epoch [38/1200] Batch [0/8] Loss D: 0.0315, Loss G: 4.9281\n",
            "Epoch [39/1200] Batch [0/8] Loss D: 0.0166, Loss G: 5.2698\n",
            "Epoch [40/1200] Batch [0/8] Loss D: 0.0514, Loss G: 5.6113\n",
            "Epoch [41/1200] Batch [0/8] Loss D: 0.0270, Loss G: 3.8707\n",
            "Epoch [42/1200] Batch [0/8] Loss D: 0.0130, Loss G: 5.1939\n",
            "Epoch [43/1200] Batch [0/8] Loss D: 0.0252, Loss G: 5.1683\n",
            "Epoch [44/1200] Batch [0/8] Loss D: 0.0326, Loss G: 4.7362\n",
            "Epoch [45/1200] Batch [0/8] Loss D: 0.0211, Loss G: 5.0380\n",
            "Epoch [46/1200] Batch [0/8] Loss D: 0.0253, Loss G: 5.1088\n",
            "Epoch [47/1200] Batch [0/8] Loss D: 0.0473, Loss G: 5.7426\n",
            "Epoch [48/1200] Batch [0/8] Loss D: 0.0456, Loss G: 4.6856\n",
            "Epoch [49/1200] Batch [0/8] Loss D: 0.0313, Loss G: 4.8198\n",
            "Epoch [50/1200] Batch [0/8] Loss D: 0.0426, Loss G: 5.1967\n",
            "Epoch [51/1200] Batch [0/8] Loss D: 0.5106, Loss G: 4.0949\n",
            "Epoch [52/1200] Batch [0/8] Loss D: 0.2441, Loss G: 4.0200\n",
            "Epoch [53/1200] Batch [0/8] Loss D: 0.1383, Loss G: 3.4418\n",
            "Epoch [54/1200] Batch [0/8] Loss D: 0.0850, Loss G: 3.8383\n",
            "Epoch [55/1200] Batch [0/8] Loss D: 0.0417, Loss G: 4.3799\n",
            "Epoch [56/1200] Batch [0/8] Loss D: 0.1848, Loss G: 2.7365\n",
            "Epoch [57/1200] Batch [0/8] Loss D: 0.0770, Loss G: 4.1141\n",
            "Epoch [58/1200] Batch [0/8] Loss D: 0.0611, Loss G: 3.9566\n",
            "Epoch [59/1200] Batch [0/8] Loss D: 0.0271, Loss G: 4.5792\n",
            "Epoch [60/1200] Batch [0/8] Loss D: 0.0293, Loss G: 4.6791\n",
            "Epoch [61/1200] Batch [0/8] Loss D: 0.0434, Loss G: 4.3995\n",
            "Epoch [62/1200] Batch [0/8] Loss D: 0.0388, Loss G: 5.1027\n",
            "Epoch [63/1200] Batch [0/8] Loss D: 0.0418, Loss G: 4.6441\n",
            "Epoch [64/1200] Batch [0/8] Loss D: 0.0402, Loss G: 5.0779\n",
            "Epoch [65/1200] Batch [0/8] Loss D: 0.0328, Loss G: 4.9986\n",
            "Epoch [66/1200] Batch [0/8] Loss D: 0.0391, Loss G: 4.5762\n",
            "Epoch [67/1200] Batch [0/8] Loss D: 0.0241, Loss G: 5.1357\n",
            "Epoch [68/1200] Batch [0/8] Loss D: 0.0251, Loss G: 4.9813\n",
            "Epoch [69/1200] Batch [0/8] Loss D: 0.0252, Loss G: 4.8023\n",
            "Epoch [70/1200] Batch [0/8] Loss D: 0.0148, Loss G: 5.3348\n",
            "Epoch [71/1200] Batch [0/8] Loss D: 0.0295, Loss G: 4.8362\n",
            "Epoch [72/1200] Batch [0/8] Loss D: 0.0261, Loss G: 4.9215\n",
            "Epoch [73/1200] Batch [0/8] Loss D: 0.0210, Loss G: 5.2850\n",
            "Epoch [74/1200] Batch [0/8] Loss D: 0.0148, Loss G: 5.2983\n",
            "Epoch [75/1200] Batch [0/8] Loss D: 0.0157, Loss G: 5.3073\n",
            "Epoch [76/1200] Batch [0/8] Loss D: 0.0120, Loss G: 5.4691\n",
            "Epoch [77/1200] Batch [0/8] Loss D: 0.0178, Loss G: 4.9958\n",
            "Epoch [78/1200] Batch [0/8] Loss D: 0.0232, Loss G: 4.8379\n",
            "Epoch [79/1200] Batch [0/8] Loss D: 0.0372, Loss G: 4.3679\n",
            "Epoch [80/1200] Batch [0/8] Loss D: 0.0123, Loss G: 5.4273\n",
            "Epoch [81/1200] Batch [0/8] Loss D: 0.0123, Loss G: 5.4448\n",
            "Epoch [82/1200] Batch [0/8] Loss D: 0.0154, Loss G: 5.8331\n",
            "Epoch [83/1200] Batch [0/8] Loss D: 0.0151, Loss G: 5.3895\n",
            "Epoch [84/1200] Batch [0/8] Loss D: 0.0244, Loss G: 4.9172\n",
            "Epoch [85/1200] Batch [0/8] Loss D: 0.0279, Loss G: 4.5157\n",
            "Epoch [86/1200] Batch [0/8] Loss D: 0.0198, Loss G: 5.3804\n",
            "Epoch [87/1200] Batch [0/8] Loss D: 0.0126, Loss G: 6.0024\n",
            "Epoch [88/1200] Batch [0/8] Loss D: 0.0085, Loss G: 5.9151\n",
            "Epoch [89/1200] Batch [0/8] Loss D: 0.0050, Loss G: 6.5812\n",
            "Epoch [90/1200] Batch [0/8] Loss D: 0.0060, Loss G: 6.3556\n",
            "Epoch [91/1200] Batch [0/8] Loss D: 0.0097, Loss G: 5.6282\n",
            "Epoch [92/1200] Batch [0/8] Loss D: 0.0273, Loss G: 4.9792\n",
            "Epoch [93/1200] Batch [0/8] Loss D: 13.0816, Loss G: 10.9521\n",
            "Epoch [94/1200] Batch [0/8] Loss D: 0.2927, Loss G: 2.3307\n",
            "Epoch [95/1200] Batch [0/8] Loss D: 0.5623, Loss G: 2.4547\n",
            "Epoch [96/1200] Batch [0/8] Loss D: 0.2528, Loss G: 2.8593\n",
            "Epoch [97/1200] Batch [0/8] Loss D: 0.1302, Loss G: 3.3152\n",
            "Epoch [98/1200] Batch [0/8] Loss D: 0.0715, Loss G: 3.8913\n",
            "Epoch [99/1200] Batch [0/8] Loss D: 0.1045, Loss G: 3.9401\n",
            "Epoch [100/1200] Batch [0/8] Loss D: 0.0927, Loss G: 3.8530\n",
            "Epoch [101/1200] Batch [0/8] Loss D: 0.0593, Loss G: 4.2302\n",
            "Epoch [102/1200] Batch [0/8] Loss D: 0.0343, Loss G: 4.5349\n",
            "Epoch [103/1200] Batch [0/8] Loss D: 0.0437, Loss G: 4.5118\n",
            "Epoch [104/1200] Batch [0/8] Loss D: 0.0623, Loss G: 4.4714\n",
            "Epoch [105/1200] Batch [0/8] Loss D: 0.0606, Loss G: 4.3823\n",
            "Epoch [106/1200] Batch [0/8] Loss D: 0.0827, Loss G: 4.0590\n",
            "Epoch [107/1200] Batch [0/8] Loss D: 0.0442, Loss G: 4.2394\n",
            "Epoch [108/1200] Batch [0/8] Loss D: 0.0264, Loss G: 4.7720\n",
            "Epoch [109/1200] Batch [0/8] Loss D: 0.0261, Loss G: 4.6458\n",
            "Epoch [110/1200] Batch [0/8] Loss D: 0.0234, Loss G: 4.5099\n",
            "Epoch [111/1200] Batch [0/8] Loss D: 0.0295, Loss G: 4.7007\n",
            "Epoch [112/1200] Batch [0/8] Loss D: 0.0480, Loss G: 4.3184\n",
            "Epoch [113/1200] Batch [0/8] Loss D: 0.0422, Loss G: 4.5093\n",
            "Epoch [114/1200] Batch [0/8] Loss D: 0.0327, Loss G: 4.6380\n",
            "Epoch [115/1200] Batch [0/8] Loss D: 0.0585, Loss G: 3.6194\n",
            "Epoch [116/1200] Batch [0/8] Loss D: 0.0483, Loss G: 4.4552\n",
            "Epoch [117/1200] Batch [0/8] Loss D: 0.0221, Loss G: 4.7853\n",
            "Epoch [118/1200] Batch [0/8] Loss D: 0.1287, Loss G: 1.0352\n",
            "Epoch [119/1200] Batch [0/8] Loss D: 0.0725, Loss G: 3.9816\n",
            "Epoch [120/1200] Batch [0/8] Loss D: 0.0695, Loss G: 4.2179\n",
            "Epoch [121/1200] Batch [0/8] Loss D: 0.0318, Loss G: 4.7010\n",
            "Epoch [122/1200] Batch [0/8] Loss D: 0.0535, Loss G: 4.2236\n",
            "Epoch [123/1200] Batch [0/8] Loss D: 0.1009, Loss G: 4.3494\n",
            "Epoch [124/1200] Batch [0/8] Loss D: 0.0653, Loss G: 4.2633\n",
            "Epoch [125/1200] Batch [0/8] Loss D: 0.1776, Loss G: 4.2926\n",
            "Epoch [126/1200] Batch [0/8] Loss D: 0.0955, Loss G: 3.7343\n",
            "Epoch [127/1200] Batch [0/8] Loss D: 0.0675, Loss G: 3.7609\n",
            "Epoch [128/1200] Batch [0/8] Loss D: 0.0384, Loss G: 4.2668\n",
            "Epoch [129/1200] Batch [0/8] Loss D: 0.0371, Loss G: 4.1491\n",
            "Epoch [130/1200] Batch [0/8] Loss D: 0.0304, Loss G: 4.5553\n",
            "Epoch [131/1200] Batch [0/8] Loss D: 0.0231, Loss G: 4.7019\n",
            "Epoch [132/1200] Batch [0/8] Loss D: 0.0313, Loss G: 4.4994\n",
            "Epoch [133/1200] Batch [0/8] Loss D: 0.0759, Loss G: 3.9117\n",
            "Epoch [134/1200] Batch [0/8] Loss D: 0.0740, Loss G: 3.5345\n",
            "Epoch [135/1200] Batch [0/8] Loss D: 0.0469, Loss G: 4.1746\n",
            "Epoch [136/1200] Batch [0/8] Loss D: 0.0271, Loss G: 4.4960\n",
            "Epoch [137/1200] Batch [0/8] Loss D: 0.0277, Loss G: 4.5420\n",
            "Epoch [138/1200] Batch [0/8] Loss D: 0.0232, Loss G: 4.8518\n",
            "Epoch [139/1200] Batch [0/8] Loss D: 0.0190, Loss G: 4.9153\n",
            "Epoch [140/1200] Batch [0/8] Loss D: 0.0395, Loss G: 4.4468\n",
            "Epoch [141/1200] Batch [0/8] Loss D: 0.0307, Loss G: 4.9239\n",
            "Epoch [142/1200] Batch [0/8] Loss D: 0.0327, Loss G: 4.7824\n",
            "Epoch [143/1200] Batch [0/8] Loss D: 0.0211, Loss G: 4.9708\n",
            "Epoch [144/1200] Batch [0/8] Loss D: 0.0128, Loss G: 5.4176\n",
            "Epoch [145/1200] Batch [0/8] Loss D: 0.0104, Loss G: 5.5352\n",
            "Epoch [146/1200] Batch [0/8] Loss D: 0.0196, Loss G: 4.9505\n",
            "Epoch [147/1200] Batch [0/8] Loss D: 0.0475, Loss G: 5.4698\n",
            "Epoch [148/1200] Batch [0/8] Loss D: 0.0413, Loss G: 4.3073\n",
            "Epoch [149/1200] Batch [0/8] Loss D: 0.0160, Loss G: 5.5507\n",
            "Epoch [150/1200] Batch [0/8] Loss D: 0.0125, Loss G: 5.3142\n",
            "Epoch [151/1200] Batch [0/8] Loss D: 0.0078, Loss G: 5.9653\n",
            "Epoch [152/1200] Batch [0/8] Loss D: 0.0095, Loss G: 5.7577\n",
            "Epoch [153/1200] Batch [0/8] Loss D: 0.0081, Loss G: 5.7209\n",
            "Epoch [154/1200] Batch [0/8] Loss D: 0.0088, Loss G: 5.7315\n",
            "Epoch [155/1200] Batch [0/8] Loss D: 0.0206, Loss G: 5.5212\n",
            "Epoch [156/1200] Batch [0/8] Loss D: 0.0282, Loss G: 4.8287\n",
            "Epoch [157/1200] Batch [0/8] Loss D: 0.0327, Loss G: 4.7742\n",
            "Epoch [158/1200] Batch [0/8] Loss D: 0.0170, Loss G: 5.7713\n",
            "Epoch [159/1200] Batch [0/8] Loss D: 0.0164, Loss G: 5.4681\n",
            "Epoch [160/1200] Batch [0/8] Loss D: 0.0190, Loss G: 4.9684\n",
            "Epoch [161/1200] Batch [0/8] Loss D: 0.0159, Loss G: 5.1158\n",
            "Epoch [162/1200] Batch [0/8] Loss D: 0.0210, Loss G: 4.9458\n",
            "Epoch [163/1200] Batch [0/8] Loss D: 0.0342, Loss G: 5.0323\n",
            "Epoch [164/1200] Batch [0/8] Loss D: 0.3735, Loss G: 3.0359\n",
            "Epoch [165/1200] Batch [0/8] Loss D: 0.1247, Loss G: 3.1335\n",
            "Epoch [166/1200] Batch [0/8] Loss D: 0.1009, Loss G: 3.4260\n",
            "Epoch [167/1200] Batch [0/8] Loss D: 0.0899, Loss G: 3.9071\n",
            "Epoch [168/1200] Batch [0/8] Loss D: 0.0870, Loss G: 4.0950\n",
            "Epoch [169/1200] Batch [0/8] Loss D: 0.2156, Loss G: 3.7083\n",
            "Epoch [170/1200] Batch [0/8] Loss D: 0.1012, Loss G: 3.7086\n",
            "Epoch [171/1200] Batch [0/8] Loss D: 0.0846, Loss G: 3.8737\n",
            "Epoch [172/1200] Batch [0/8] Loss D: 0.0528, Loss G: 4.2169\n",
            "Epoch [173/1200] Batch [0/8] Loss D: 0.0513, Loss G: 4.1064\n",
            "Epoch [174/1200] Batch [0/8] Loss D: 0.0635, Loss G: 3.5737\n",
            "Epoch [175/1200] Batch [0/8] Loss D: 0.0436, Loss G: 4.3484\n",
            "Epoch [176/1200] Batch [0/8] Loss D: 0.0462, Loss G: 4.2886\n",
            "Epoch [177/1200] Batch [0/8] Loss D: 0.0386, Loss G: 4.4227\n",
            "Epoch [178/1200] Batch [0/8] Loss D: 0.0817, Loss G: 4.1247\n",
            "Epoch [179/1200] Batch [0/8] Loss D: 0.0548, Loss G: 4.4300\n",
            "Epoch [180/1200] Batch [0/8] Loss D: 0.0474, Loss G: 4.0804\n",
            "Epoch [181/1200] Batch [0/8] Loss D: 0.0411, Loss G: 4.3983\n",
            "Epoch [182/1200] Batch [0/8] Loss D: 0.0434, Loss G: 4.4482\n",
            "Epoch [183/1200] Batch [0/8] Loss D: 0.0310, Loss G: 4.6437\n",
            "Epoch [184/1200] Batch [0/8] Loss D: 0.0306, Loss G: 4.6078\n",
            "Epoch [185/1200] Batch [0/8] Loss D: 0.0242, Loss G: 4.8040\n",
            "Epoch [186/1200] Batch [0/8] Loss D: 0.0248, Loss G: 4.7754\n",
            "Epoch [187/1200] Batch [0/8] Loss D: 0.0338, Loss G: 4.7273\n",
            "Epoch [188/1200] Batch [0/8] Loss D: 0.1009, Loss G: 4.9530\n",
            "Epoch [189/1200] Batch [0/8] Loss D: 0.1610, Loss G: 2.7357\n",
            "Epoch [190/1200] Batch [0/8] Loss D: 0.0791, Loss G: 4.0845\n",
            "Epoch [191/1200] Batch [0/8] Loss D: 0.1439, Loss G: 3.2442\n",
            "Epoch [192/1200] Batch [0/8] Loss D: 0.0718, Loss G: 3.6614\n",
            "Epoch [193/1200] Batch [0/8] Loss D: 0.0520, Loss G: 4.0915\n",
            "Epoch [194/1200] Batch [0/8] Loss D: 0.0264, Loss G: 4.6255\n",
            "Epoch [195/1200] Batch [0/8] Loss D: 0.0230, Loss G: 4.7077\n",
            "Epoch [196/1200] Batch [0/8] Loss D: 0.0506, Loss G: 4.1315\n",
            "Epoch [197/1200] Batch [0/8] Loss D: 0.0586, Loss G: 4.4669\n",
            "Epoch [198/1200] Batch [0/8] Loss D: 0.0334, Loss G: 4.4494\n",
            "Epoch [199/1200] Batch [0/8] Loss D: 0.0280, Loss G: 4.5978\n",
            "Epoch [200/1200] Batch [0/8] Loss D: 0.0363, Loss G: 4.4103\n",
            "Epoch [201/1200] Batch [0/8] Loss D: 0.0200, Loss G: 4.8112\n",
            "Epoch [202/1200] Batch [0/8] Loss D: 0.0212, Loss G: 4.4803\n",
            "Epoch [203/1200] Batch [0/8] Loss D: 0.0128, Loss G: 5.2178\n",
            "Epoch [204/1200] Batch [0/8] Loss D: 0.0130, Loss G: 5.3676\n",
            "Epoch [205/1200] Batch [0/8] Loss D: 0.0138, Loss G: 5.3163\n",
            "Epoch [206/1200] Batch [0/8] Loss D: 0.0183, Loss G: 4.8322\n",
            "Epoch [207/1200] Batch [0/8] Loss D: 0.0711, Loss G: 3.7590\n",
            "Epoch [208/1200] Batch [0/8] Loss D: 0.0650, Loss G: 3.6952\n",
            "Epoch [209/1200] Batch [0/8] Loss D: 0.3002, Loss G: 2.5077\n",
            "Epoch [210/1200] Batch [0/8] Loss D: 0.2339, Loss G: 3.0541\n",
            "Epoch [211/1200] Batch [0/8] Loss D: 0.1327, Loss G: 3.6525\n",
            "Epoch [212/1200] Batch [0/8] Loss D: 0.1043, Loss G: 3.7231\n",
            "Epoch [213/1200] Batch [0/8] Loss D: 0.0847, Loss G: 3.3787\n",
            "Epoch [214/1200] Batch [0/8] Loss D: 0.0595, Loss G: 3.7120\n",
            "Epoch [215/1200] Batch [0/8] Loss D: 0.1013, Loss G: 3.4772\n",
            "Epoch [216/1200] Batch [0/8] Loss D: 0.0473, Loss G: 3.8072\n",
            "Epoch [217/1200] Batch [0/8] Loss D: 0.0394, Loss G: 4.1493\n",
            "Epoch [218/1200] Batch [0/8] Loss D: 0.0431, Loss G: 4.1231\n",
            "Epoch [219/1200] Batch [0/8] Loss D: 0.0396, Loss G: 4.4383\n",
            "Epoch [220/1200] Batch [0/8] Loss D: 0.1276, Loss G: 3.4090\n",
            "Epoch [221/1200] Batch [0/8] Loss D: 0.0588, Loss G: 3.8271\n",
            "Epoch [222/1200] Batch [0/8] Loss D: 0.0456, Loss G: 4.3996\n",
            "Epoch [223/1200] Batch [0/8] Loss D: 0.0433, Loss G: 4.2294\n",
            "Epoch [224/1200] Batch [0/8] Loss D: 0.0342, Loss G: 4.3217\n",
            "Epoch [225/1200] Batch [0/8] Loss D: 0.0426, Loss G: 4.2631\n",
            "Epoch [226/1200] Batch [0/8] Loss D: 0.6726, Loss G: 0.7427\n",
            "Epoch [227/1200] Batch [0/8] Loss D: 0.0400, Loss G: 4.9152\n",
            "Epoch [228/1200] Batch [0/8] Loss D: 0.0555, Loss G: 4.1810\n",
            "Epoch [229/1200] Batch [0/8] Loss D: 0.0282, Loss G: 4.5541\n",
            "Epoch [230/1200] Batch [0/8] Loss D: 0.0571, Loss G: 4.1732\n",
            "Epoch [231/1200] Batch [0/8] Loss D: 0.0389, Loss G: 4.4330\n",
            "Epoch [232/1200] Batch [0/8] Loss D: 0.0299, Loss G: 4.7318\n",
            "Epoch [233/1200] Batch [0/8] Loss D: 0.0240, Loss G: 4.8305\n",
            "Epoch [234/1200] Batch [0/8] Loss D: 0.0196, Loss G: 5.5040\n",
            "Epoch [235/1200] Batch [0/8] Loss D: 0.0125, Loss G: 5.5306\n",
            "Epoch [236/1200] Batch [0/8] Loss D: 0.0097, Loss G: 5.5426\n",
            "Epoch [237/1200] Batch [0/8] Loss D: 0.0259, Loss G: 4.7814\n",
            "Epoch [238/1200] Batch [0/8] Loss D: 0.1692, Loss G: 5.6926\n",
            "Epoch [239/1200] Batch [0/8] Loss D: 0.1769, Loss G: 4.3545\n",
            "Epoch [240/1200] Batch [0/8] Loss D: 0.1228, Loss G: 4.0880\n",
            "Epoch [241/1200] Batch [0/8] Loss D: 0.1315, Loss G: 3.2816\n",
            "Epoch [242/1200] Batch [0/8] Loss D: 0.0873, Loss G: 3.4049\n",
            "Epoch [243/1200] Batch [0/8] Loss D: 0.0431, Loss G: 4.1014\n",
            "Epoch [244/1200] Batch [0/8] Loss D: 0.0420, Loss G: 4.1328\n",
            "Epoch [245/1200] Batch [0/8] Loss D: 0.0357, Loss G: 4.3153\n",
            "Epoch [246/1200] Batch [0/8] Loss D: 0.0326, Loss G: 4.3382\n",
            "Epoch [247/1200] Batch [0/8] Loss D: 0.0269, Loss G: 4.2304\n",
            "Epoch [248/1200] Batch [0/8] Loss D: 0.0297, Loss G: 4.3448\n",
            "Epoch [249/1200] Batch [0/8] Loss D: 0.0141, Loss G: 5.0268\n",
            "Epoch [250/1200] Batch [0/8] Loss D: 0.0311, Loss G: 4.4644\n",
            "Epoch [251/1200] Batch [0/8] Loss D: 0.0613, Loss G: 3.7876\n",
            "Epoch [252/1200] Batch [0/8] Loss D: 0.0661, Loss G: 3.5187\n",
            "Epoch [253/1200] Batch [0/8] Loss D: 0.0753, Loss G: 3.8201\n",
            "Epoch [254/1200] Batch [0/8] Loss D: 0.0379, Loss G: 4.3886\n",
            "Epoch [255/1200] Batch [0/8] Loss D: 0.0239, Loss G: 4.8329\n",
            "Epoch [256/1200] Batch [0/8] Loss D: 0.0338, Loss G: 4.3324\n",
            "Epoch [257/1200] Batch [0/8] Loss D: 0.0305, Loss G: 4.5120\n",
            "Epoch [258/1200] Batch [0/8] Loss D: 0.0312, Loss G: 4.6752\n",
            "Epoch [259/1200] Batch [0/8] Loss D: 0.0157, Loss G: 5.2370\n",
            "Epoch [260/1200] Batch [0/8] Loss D: 0.0281, Loss G: 4.4115\n",
            "Epoch [261/1200] Batch [0/8] Loss D: 0.3238, Loss G: 3.5346\n",
            "Epoch [262/1200] Batch [0/8] Loss D: 0.0676, Loss G: 3.5699\n",
            "Epoch [263/1200] Batch [0/8] Loss D: 0.0227, Loss G: 5.0496\n",
            "Epoch [264/1200] Batch [0/8] Loss D: 0.0321, Loss G: 4.8784\n",
            "Epoch [265/1200] Batch [0/8] Loss D: 0.0317, Loss G: 4.5087\n",
            "Epoch [266/1200] Batch [0/8] Loss D: 0.0200, Loss G: 4.9715\n",
            "Epoch [267/1200] Batch [0/8] Loss D: 0.2305, Loss G: 1.5861\n",
            "Epoch [268/1200] Batch [0/8] Loss D: 0.1135, Loss G: 4.4489\n",
            "Epoch [269/1200] Batch [0/8] Loss D: 0.0709, Loss G: 3.4242\n",
            "Epoch [270/1200] Batch [0/8] Loss D: 0.0428, Loss G: 3.9796\n",
            "Epoch [271/1200] Batch [0/8] Loss D: 0.0578, Loss G: 4.0300\n",
            "Epoch [272/1200] Batch [0/8] Loss D: 0.0249, Loss G: 4.4807\n",
            "Epoch [273/1200] Batch [0/8] Loss D: 0.0280, Loss G: 4.4550\n",
            "Epoch [274/1200] Batch [0/8] Loss D: 0.0755, Loss G: 3.6727\n",
            "Epoch [275/1200] Batch [0/8] Loss D: 0.0626, Loss G: 4.1513\n",
            "Epoch [276/1200] Batch [0/8] Loss D: 0.0299, Loss G: 4.5861\n",
            "Epoch [277/1200] Batch [0/8] Loss D: 0.0994, Loss G: 3.7620\n",
            "Epoch [278/1200] Batch [0/8] Loss D: 0.0527, Loss G: 3.9647\n",
            "Epoch [279/1200] Batch [0/8] Loss D: 0.0396, Loss G: 4.2702\n",
            "Epoch [280/1200] Batch [0/8] Loss D: 0.0357, Loss G: 4.1480\n",
            "Epoch [281/1200] Batch [0/8] Loss D: 0.0228, Loss G: 4.6974\n",
            "Epoch [282/1200] Batch [0/8] Loss D: 0.0170, Loss G: 4.7705\n",
            "Epoch [283/1200] Batch [0/8] Loss D: 0.0312, Loss G: 4.6253\n",
            "Epoch [284/1200] Batch [0/8] Loss D: 0.0441, Loss G: 4.5884\n",
            "Epoch [285/1200] Batch [0/8] Loss D: 0.0833, Loss G: 3.2465\n",
            "Epoch [286/1200] Batch [0/8] Loss D: 0.2674, Loss G: 1.2826\n",
            "Epoch [287/1200] Batch [0/8] Loss D: 0.1446, Loss G: 3.1392\n",
            "Epoch [288/1200] Batch [0/8] Loss D: 0.0988, Loss G: 3.4762\n",
            "Epoch [289/1200] Batch [0/8] Loss D: 0.0625, Loss G: 3.9345\n",
            "Epoch [290/1200] Batch [0/8] Loss D: 0.0442, Loss G: 4.1940\n",
            "Epoch [291/1200] Batch [0/8] Loss D: 0.0341, Loss G: 4.1275\n",
            "Epoch [292/1200] Batch [0/8] Loss D: 0.0307, Loss G: 4.3317\n",
            "Epoch [293/1200] Batch [0/8] Loss D: 0.0251, Loss G: 4.6278\n",
            "Epoch [294/1200] Batch [0/8] Loss D: 0.0728, Loss G: 3.9297\n",
            "Epoch [295/1200] Batch [0/8] Loss D: 0.0370, Loss G: 4.2025\n",
            "Epoch [296/1200] Batch [0/8] Loss D: 0.0174, Loss G: 5.0028\n",
            "Epoch [297/1200] Batch [0/8] Loss D: 0.0293, Loss G: 4.5434\n",
            "Epoch [298/1200] Batch [0/8] Loss D: 0.0505, Loss G: 3.9963\n",
            "Epoch [299/1200] Batch [0/8] Loss D: 0.0362, Loss G: 4.5610\n",
            "Epoch [300/1200] Batch [0/8] Loss D: 0.0530, Loss G: 4.0896\n",
            "Epoch [301/1200] Batch [0/8] Loss D: 0.0261, Loss G: 4.7727\n",
            "Epoch [302/1200] Batch [0/8] Loss D: 0.0218, Loss G: 4.9086\n",
            "Epoch [303/1200] Batch [0/8] Loss D: 0.0121, Loss G: 5.4648\n",
            "Epoch [304/1200] Batch [0/8] Loss D: 0.0120, Loss G: 5.3558\n",
            "Epoch [305/1200] Batch [0/8] Loss D: 0.0117, Loss G: 5.4431\n",
            "Epoch [306/1200] Batch [0/8] Loss D: 0.0169, Loss G: 5.1490\n",
            "Epoch [307/1200] Batch [0/8] Loss D: 0.0255, Loss G: 5.5657\n",
            "Epoch [308/1200] Batch [0/8] Loss D: 0.0129, Loss G: 5.1474\n",
            "Epoch [309/1200] Batch [0/8] Loss D: 0.0455, Loss G: 4.5519\n",
            "Epoch [310/1200] Batch [0/8] Loss D: 0.0117, Loss G: 5.0298\n",
            "Epoch [311/1200] Batch [0/8] Loss D: 0.0153, Loss G: 4.6798\n",
            "Epoch [312/1200] Batch [0/8] Loss D: 0.0118, Loss G: 5.6546\n",
            "Epoch [313/1200] Batch [0/8] Loss D: 0.0077, Loss G: 5.7395\n",
            "Epoch [314/1200] Batch [0/8] Loss D: 0.0053, Loss G: 6.1040\n",
            "Epoch [315/1200] Batch [0/8] Loss D: 0.0110, Loss G: 5.4459\n",
            "Epoch [316/1200] Batch [0/8] Loss D: 0.3184, Loss G: 0.1760\n",
            "Epoch [317/1200] Batch [0/8] Loss D: 0.0700, Loss G: 4.5938\n",
            "Epoch [318/1200] Batch [0/8] Loss D: 0.1252, Loss G: 3.9978\n",
            "Epoch [319/1200] Batch [0/8] Loss D: 0.1574, Loss G: 3.5989\n",
            "Epoch [320/1200] Batch [0/8] Loss D: 0.0697, Loss G: 6.2348\n",
            "Epoch [321/1200] Batch [0/8] Loss D: 0.0258, Loss G: 5.1493\n",
            "Epoch [322/1200] Batch [0/8] Loss D: 0.0335, Loss G: 4.5050\n",
            "Epoch [323/1200] Batch [0/8] Loss D: 0.0519, Loss G: 5.0216\n",
            "Epoch [324/1200] Batch [0/8] Loss D: 0.0641, Loss G: 4.3024\n",
            "Epoch [325/1200] Batch [0/8] Loss D: 0.1132, Loss G: 3.6970\n",
            "Epoch [326/1200] Batch [0/8] Loss D: 0.0408, Loss G: 4.3347\n",
            "Epoch [327/1200] Batch [0/8] Loss D: 0.0211, Loss G: 4.8682\n",
            "Epoch [328/1200] Batch [0/8] Loss D: 0.0124, Loss G: 5.1643\n",
            "Epoch [329/1200] Batch [0/8] Loss D: 0.0130, Loss G: 5.3115\n",
            "Epoch [330/1200] Batch [0/8] Loss D: 0.0094, Loss G: 5.5609\n",
            "Epoch [331/1200] Batch [0/8] Loss D: 0.0218, Loss G: 4.7446\n",
            "Epoch [332/1200] Batch [0/8] Loss D: 0.0665, Loss G: 4.4709\n",
            "Epoch [333/1200] Batch [0/8] Loss D: 0.0295, Loss G: 4.8369\n",
            "Epoch [334/1200] Batch [0/8] Loss D: 0.0208, Loss G: 4.7048\n",
            "Epoch [335/1200] Batch [0/8] Loss D: 0.0181, Loss G: 4.8954\n",
            "Epoch [336/1200] Batch [0/8] Loss D: 0.0169, Loss G: 5.1544\n",
            "Epoch [337/1200] Batch [0/8] Loss D: 0.0143, Loss G: 5.2812\n",
            "Epoch [338/1200] Batch [0/8] Loss D: 0.0110, Loss G: 5.1378\n",
            "Epoch [339/1200] Batch [0/8] Loss D: 0.0138, Loss G: 5.1774\n",
            "Epoch [340/1200] Batch [0/8] Loss D: 0.0065, Loss G: 5.9253\n",
            "Epoch [341/1200] Batch [0/8] Loss D: 0.0101, Loss G: 5.5194\n",
            "Epoch [342/1200] Batch [0/8] Loss D: 0.0069, Loss G: 5.9678\n",
            "Epoch [343/1200] Batch [0/8] Loss D: 0.0073, Loss G: 5.6502\n",
            "Epoch [344/1200] Batch [0/8] Loss D: 0.0085, Loss G: 5.8761\n",
            "Epoch [345/1200] Batch [0/8] Loss D: 0.0057, Loss G: 6.1628\n",
            "Epoch [346/1200] Batch [0/8] Loss D: 0.0060, Loss G: 6.0181\n",
            "Epoch [347/1200] Batch [0/8] Loss D: 0.0042, Loss G: 6.0222\n",
            "Epoch [348/1200] Batch [0/8] Loss D: 0.0043, Loss G: 6.0457\n",
            "Epoch [349/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.3312\n",
            "Epoch [350/1200] Batch [0/8] Loss D: 0.0070, Loss G: 6.0233\n",
            "Epoch [351/1200] Batch [0/8] Loss D: 0.0120, Loss G: 5.4073\n",
            "Epoch [352/1200] Batch [0/8] Loss D: 0.0092, Loss G: 5.5923\n",
            "Epoch [353/1200] Batch [0/8] Loss D: 0.0106, Loss G: 5.9939\n",
            "Epoch [354/1200] Batch [0/8] Loss D: 0.0083, Loss G: 6.0685\n",
            "Epoch [355/1200] Batch [0/8] Loss D: 0.0043, Loss G: 6.0542\n",
            "Epoch [356/1200] Batch [0/8] Loss D: 0.0053, Loss G: 6.0270\n",
            "Epoch [357/1200] Batch [0/8] Loss D: 0.0061, Loss G: 6.2983\n",
            "Epoch [358/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.3967\n",
            "Epoch [359/1200] Batch [0/8] Loss D: 0.0052, Loss G: 5.9909\n",
            "Epoch [360/1200] Batch [0/8] Loss D: 0.0076, Loss G: 6.1515\n",
            "Epoch [361/1200] Batch [0/8] Loss D: 0.0113, Loss G: 5.4984\n",
            "Epoch [362/1200] Batch [0/8] Loss D: 0.0122, Loss G: 5.3363\n",
            "Epoch [363/1200] Batch [0/8] Loss D: 0.2797, Loss G: 0.4179\n",
            "Epoch [364/1200] Batch [0/8] Loss D: 0.0445, Loss G: 5.1580\n",
            "Epoch [365/1200] Batch [0/8] Loss D: 0.0621, Loss G: 4.6973\n",
            "Epoch [366/1200] Batch [0/8] Loss D: 0.0471, Loss G: 4.4966\n",
            "Epoch [367/1200] Batch [0/8] Loss D: 0.0274, Loss G: 4.4474\n",
            "Epoch [368/1200] Batch [0/8] Loss D: 0.0985, Loss G: 3.8821\n",
            "Epoch [369/1200] Batch [0/8] Loss D: 0.0676, Loss G: 3.9459\n",
            "Epoch [370/1200] Batch [0/8] Loss D: 0.0556, Loss G: 3.7241\n",
            "Epoch [371/1200] Batch [0/8] Loss D: 0.0281, Loss G: 4.3995\n",
            "Epoch [372/1200] Batch [0/8] Loss D: 0.0182, Loss G: 4.7299\n",
            "Epoch [373/1200] Batch [0/8] Loss D: 0.0190, Loss G: 4.7755\n",
            "Epoch [374/1200] Batch [0/8] Loss D: 0.0246, Loss G: 4.8631\n",
            "Epoch [375/1200] Batch [0/8] Loss D: 0.2306, Loss G: 2.9016\n",
            "Epoch [376/1200] Batch [0/8] Loss D: 0.0422, Loss G: 4.5220\n",
            "Epoch [377/1200] Batch [0/8] Loss D: 0.0746, Loss G: 4.1029\n",
            "Epoch [378/1200] Batch [0/8] Loss D: 0.0395, Loss G: 3.7385\n",
            "Epoch [379/1200] Batch [0/8] Loss D: 0.0264, Loss G: 4.5741\n",
            "Epoch [380/1200] Batch [0/8] Loss D: 0.0196, Loss G: 5.9419\n",
            "Epoch [381/1200] Batch [0/8] Loss D: 0.0165, Loss G: 4.9420\n",
            "Epoch [382/1200] Batch [0/8] Loss D: 0.0290, Loss G: 4.3526\n",
            "Epoch [383/1200] Batch [0/8] Loss D: 0.0144, Loss G: 5.0958\n",
            "Epoch [384/1200] Batch [0/8] Loss D: 0.0288, Loss G: 5.0961\n",
            "Epoch [385/1200] Batch [0/8] Loss D: 0.0183, Loss G: 4.9600\n",
            "Epoch [386/1200] Batch [0/8] Loss D: 0.0143, Loss G: 5.0259\n",
            "Epoch [387/1200] Batch [0/8] Loss D: 0.0182, Loss G: 5.5280\n",
            "Epoch [388/1200] Batch [0/8] Loss D: 0.0116, Loss G: 5.3940\n",
            "Epoch [389/1200] Batch [0/8] Loss D: 0.0111, Loss G: 5.6816\n",
            "Epoch [390/1200] Batch [0/8] Loss D: 0.0097, Loss G: 5.5914\n",
            "Epoch [391/1200] Batch [0/8] Loss D: 0.0139, Loss G: 5.5743\n",
            "Epoch [392/1200] Batch [0/8] Loss D: 0.0077, Loss G: 5.9635\n",
            "Epoch [393/1200] Batch [0/8] Loss D: 0.0186, Loss G: 4.7909\n",
            "Epoch [394/1200] Batch [0/8] Loss D: 0.0163, Loss G: 5.4526\n",
            "Epoch [395/1200] Batch [0/8] Loss D: 0.0117, Loss G: 5.1137\n",
            "Epoch [396/1200] Batch [0/8] Loss D: 0.0106, Loss G: 5.5369\n",
            "Epoch [397/1200] Batch [0/8] Loss D: 0.0072, Loss G: 6.0472\n",
            "Epoch [398/1200] Batch [0/8] Loss D: 0.0059, Loss G: 6.0419\n",
            "Epoch [399/1200] Batch [0/8] Loss D: 0.0368, Loss G: 5.3253\n",
            "Epoch [400/1200] Batch [0/8] Loss D: 1.9151, Loss G: 7.5862\n",
            "Epoch [401/1200] Batch [0/8] Loss D: 0.0297, Loss G: 4.5148\n",
            "Epoch [402/1200] Batch [0/8] Loss D: 0.0187, Loss G: 4.7991\n",
            "Epoch [403/1200] Batch [0/8] Loss D: 0.0212, Loss G: 4.8157\n",
            "Epoch [404/1200] Batch [0/8] Loss D: 0.0240, Loss G: 5.0178\n",
            "Epoch [405/1200] Batch [0/8] Loss D: 0.0132, Loss G: 5.1405\n",
            "Epoch [406/1200] Batch [0/8] Loss D: 0.0150, Loss G: 5.2797\n",
            "Epoch [407/1200] Batch [0/8] Loss D: 0.0267, Loss G: 4.9825\n",
            "Epoch [408/1200] Batch [0/8] Loss D: 0.1613, Loss G: 4.5804\n",
            "Epoch [409/1200] Batch [0/8] Loss D: 0.0542, Loss G: 3.9640\n",
            "Epoch [410/1200] Batch [0/8] Loss D: 0.0437, Loss G: 4.6322\n",
            "Epoch [411/1200] Batch [0/8] Loss D: 0.0394, Loss G: 4.4856\n",
            "Epoch [412/1200] Batch [0/8] Loss D: 0.0317, Loss G: 4.4264\n",
            "Epoch [413/1200] Batch [0/8] Loss D: 0.0172, Loss G: 4.8909\n",
            "Epoch [414/1200] Batch [0/8] Loss D: 0.0152, Loss G: 5.2466\n",
            "Epoch [415/1200] Batch [0/8] Loss D: 0.0197, Loss G: 4.9043\n",
            "Epoch [416/1200] Batch [0/8] Loss D: 0.0249, Loss G: 5.0328\n",
            "Epoch [417/1200] Batch [0/8] Loss D: 0.0081, Loss G: 5.7980\n",
            "Epoch [418/1200] Batch [0/8] Loss D: 0.0077, Loss G: 5.7110\n",
            "Epoch [419/1200] Batch [0/8] Loss D: 0.0092, Loss G: 5.7065\n",
            "Epoch [420/1200] Batch [0/8] Loss D: 0.0141, Loss G: 5.4419\n",
            "Epoch [421/1200] Batch [0/8] Loss D: 0.0334, Loss G: 4.5335\n",
            "Epoch [422/1200] Batch [0/8] Loss D: 0.0264, Loss G: 4.8525\n",
            "Epoch [423/1200] Batch [0/8] Loss D: 0.0224, Loss G: 4.5649\n",
            "Epoch [424/1200] Batch [0/8] Loss D: 0.0248, Loss G: 4.7198\n",
            "Epoch [425/1200] Batch [0/8] Loss D: 0.0236, Loss G: 4.4841\n",
            "Epoch [426/1200] Batch [0/8] Loss D: 0.0154, Loss G: 5.1096\n",
            "Epoch [427/1200] Batch [0/8] Loss D: 0.0102, Loss G: 5.3820\n",
            "Epoch [428/1200] Batch [0/8] Loss D: 0.0112, Loss G: 5.7460\n",
            "Epoch [429/1200] Batch [0/8] Loss D: 0.0155, Loss G: 4.9486\n",
            "Epoch [430/1200] Batch [0/8] Loss D: 0.0062, Loss G: 6.1023\n",
            "Epoch [431/1200] Batch [0/8] Loss D: 0.0067, Loss G: 5.9704\n",
            "Epoch [432/1200] Batch [0/8] Loss D: 0.0069, Loss G: 5.8868\n",
            "Epoch [433/1200] Batch [0/8] Loss D: 0.0098, Loss G: 5.6199\n",
            "Epoch [434/1200] Batch [0/8] Loss D: 0.0044, Loss G: 6.5238\n",
            "Epoch [435/1200] Batch [0/8] Loss D: 0.0049, Loss G: 6.3714\n",
            "Epoch [436/1200] Batch [0/8] Loss D: 0.0064, Loss G: 6.1686\n",
            "Epoch [437/1200] Batch [0/8] Loss D: 0.0051, Loss G: 6.1607\n",
            "Epoch [438/1200] Batch [0/8] Loss D: 0.0105, Loss G: 6.1317\n",
            "Epoch [439/1200] Batch [0/8] Loss D: 0.0219, Loss G: 5.1303\n",
            "Epoch [440/1200] Batch [0/8] Loss D: 0.0215, Loss G: 4.9057\n",
            "Epoch [441/1200] Batch [0/8] Loss D: 0.4840, Loss G: 5.0264\n",
            "Epoch [442/1200] Batch [0/8] Loss D: 0.0793, Loss G: 3.8808\n",
            "Epoch [443/1200] Batch [0/8] Loss D: 0.0761, Loss G: 4.1042\n",
            "Epoch [444/1200] Batch [0/8] Loss D: 0.1024, Loss G: 4.1162\n",
            "Epoch [445/1200] Batch [0/8] Loss D: 0.0484, Loss G: 4.3919\n",
            "Epoch [446/1200] Batch [0/8] Loss D: 0.0460, Loss G: 4.1082\n",
            "Epoch [447/1200] Batch [0/8] Loss D: 0.0256, Loss G: 4.8762\n",
            "Epoch [448/1200] Batch [0/8] Loss D: 0.0157, Loss G: 5.0432\n",
            "Epoch [449/1200] Batch [0/8] Loss D: 0.0178, Loss G: 5.2125\n",
            "Epoch [450/1200] Batch [0/8] Loss D: 0.0120, Loss G: 5.4353\n",
            "Epoch [451/1200] Batch [0/8] Loss D: 0.0072, Loss G: 5.7682\n",
            "Epoch [452/1200] Batch [0/8] Loss D: 0.0086, Loss G: 5.4832\n",
            "Epoch [453/1200] Batch [0/8] Loss D: 0.0246, Loss G: 5.0674\n",
            "Epoch [454/1200] Batch [0/8] Loss D: 0.0328, Loss G: 4.4817\n",
            "Epoch [455/1200] Batch [0/8] Loss D: 0.0234, Loss G: 4.8503\n",
            "Epoch [456/1200] Batch [0/8] Loss D: 0.1123, Loss G: 4.7235\n",
            "Epoch [457/1200] Batch [0/8] Loss D: 0.0658, Loss G: 4.3150\n",
            "Epoch [458/1200] Batch [0/8] Loss D: 0.0219, Loss G: 5.0305\n",
            "Epoch [459/1200] Batch [0/8] Loss D: 0.0123, Loss G: 5.3159\n",
            "Epoch [460/1200] Batch [0/8] Loss D: 0.0160, Loss G: 5.3626\n",
            "Epoch [461/1200] Batch [0/8] Loss D: 0.0079, Loss G: 5.4684\n",
            "Epoch [462/1200] Batch [0/8] Loss D: 0.0108, Loss G: 5.1966\n",
            "Epoch [463/1200] Batch [0/8] Loss D: 0.0070, Loss G: 5.8471\n",
            "Epoch [464/1200] Batch [0/8] Loss D: 0.0075, Loss G: 5.7894\n",
            "Epoch [465/1200] Batch [0/8] Loss D: 0.0063, Loss G: 6.1237\n",
            "Epoch [466/1200] Batch [0/8] Loss D: 0.0047, Loss G: 5.9896\n",
            "Epoch [467/1200] Batch [0/8] Loss D: 0.0034, Loss G: 6.2994\n",
            "Epoch [468/1200] Batch [0/8] Loss D: 0.0048, Loss G: 6.0322\n",
            "Epoch [469/1200] Batch [0/8] Loss D: 0.0042, Loss G: 6.3623\n",
            "Epoch [470/1200] Batch [0/8] Loss D: 0.0038, Loss G: 6.2571\n",
            "Epoch [471/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.3514\n",
            "Epoch [472/1200] Batch [0/8] Loss D: 0.0042, Loss G: 6.2345\n",
            "Epoch [473/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.5181\n",
            "Epoch [474/1200] Batch [0/8] Loss D: 0.0039, Loss G: 6.4150\n",
            "Epoch [475/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.3871\n",
            "Epoch [476/1200] Batch [0/8] Loss D: 0.0028, Loss G: 6.5058\n",
            "Epoch [477/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.7588\n",
            "Epoch [478/1200] Batch [0/8] Loss D: 0.0028, Loss G: 6.6637\n",
            "Epoch [479/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.6735\n",
            "Epoch [480/1200] Batch [0/8] Loss D: 0.0022, Loss G: 6.6783\n",
            "Epoch [481/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.7479\n",
            "Epoch [482/1200] Batch [0/8] Loss D: 0.0036, Loss G: 6.8452\n",
            "Epoch [483/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.8023\n",
            "Epoch [484/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.6330\n",
            "Epoch [485/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.8523\n",
            "Epoch [486/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.2848\n",
            "Epoch [487/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.4234\n",
            "Epoch [488/1200] Batch [0/8] Loss D: 0.0033, Loss G: 6.3337\n",
            "Epoch [489/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.6777\n",
            "Epoch [490/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.1272\n",
            "Epoch [491/1200] Batch [0/8] Loss D: 0.0035, Loss G: 6.4198\n",
            "Epoch [492/1200] Batch [0/8] Loss D: 0.0051, Loss G: 6.6021\n",
            "Epoch [493/1200] Batch [0/8] Loss D: 0.0019, Loss G: 6.7898\n",
            "Epoch [494/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.5103\n",
            "Epoch [495/1200] Batch [0/8] Loss D: 0.0055, Loss G: 5.8589\n",
            "Epoch [496/1200] Batch [0/8] Loss D: 0.0051, Loss G: 6.1104\n",
            "Epoch [497/1200] Batch [0/8] Loss D: 0.0112, Loss G: 5.8763\n",
            "Epoch [498/1200] Batch [0/8] Loss D: 0.0138, Loss G: 5.8970\n",
            "Epoch [499/1200] Batch [0/8] Loss D: 0.0110, Loss G: 5.1518\n",
            "Epoch [500/1200] Batch [0/8] Loss D: 0.0065, Loss G: 5.6190\n",
            "Epoch [501/1200] Batch [0/8] Loss D: 0.0169, Loss G: 5.2621\n",
            "Epoch [502/1200] Batch [0/8] Loss D: 0.0082, Loss G: 5.5400\n",
            "Epoch [503/1200] Batch [0/8] Loss D: 0.0111, Loss G: 5.2602\n",
            "Epoch [504/1200] Batch [0/8] Loss D: 0.0060, Loss G: 5.9546\n",
            "Epoch [505/1200] Batch [0/8] Loss D: 0.0068, Loss G: 6.1993\n",
            "Epoch [506/1200] Batch [0/8] Loss D: 0.0039, Loss G: 6.0466\n",
            "Epoch [507/1200] Batch [0/8] Loss D: 0.0048, Loss G: 6.4886\n",
            "Epoch [508/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.5136\n",
            "Epoch [509/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.5823\n",
            "Epoch [510/1200] Batch [0/8] Loss D: 0.0037, Loss G: 6.2410\n",
            "Epoch [511/1200] Batch [0/8] Loss D: 0.0047, Loss G: 6.3427\n",
            "Epoch [512/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.3591\n",
            "Epoch [513/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.6350\n",
            "Epoch [514/1200] Batch [0/8] Loss D: 0.0031, Loss G: 6.7466\n",
            "Epoch [515/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.7376\n",
            "Epoch [516/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.5343\n",
            "Epoch [517/1200] Batch [0/8] Loss D: 0.0028, Loss G: 6.7163\n",
            "Epoch [518/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.7844\n",
            "Epoch [519/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.5250\n",
            "Epoch [520/1200] Batch [0/8] Loss D: 0.0019, Loss G: 7.0403\n",
            "Epoch [521/1200] Batch [0/8] Loss D: 0.0022, Loss G: 6.7160\n",
            "Epoch [522/1200] Batch [0/8] Loss D: 0.0018, Loss G: 6.7620\n",
            "Epoch [523/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.6321\n",
            "Epoch [524/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.0412\n",
            "Epoch [525/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.8411\n",
            "Epoch [526/1200] Batch [0/8] Loss D: 0.0026, Loss G: 6.7247\n",
            "Epoch [527/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.8400\n",
            "Epoch [528/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.2874\n",
            "Epoch [529/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.8187\n",
            "Epoch [530/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.4987\n",
            "Epoch [531/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.6448\n",
            "Epoch [532/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.3447\n",
            "Epoch [533/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.0915\n",
            "Epoch [534/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.2967\n",
            "Epoch [535/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.2729\n",
            "Epoch [536/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.6484\n",
            "Epoch [537/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.0747\n",
            "Epoch [538/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.7158\n",
            "Epoch [539/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.4520\n",
            "Epoch [540/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.3245\n",
            "Epoch [541/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.1797\n",
            "Epoch [542/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.0921\n",
            "Epoch [543/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.2660\n",
            "Epoch [544/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.1391\n",
            "Epoch [545/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.4555\n",
            "Epoch [546/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.6340\n",
            "Epoch [547/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.3895\n",
            "Epoch [548/1200] Batch [0/8] Loss D: 0.0019, Loss G: 7.4867\n",
            "Epoch [549/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.5924\n",
            "Epoch [550/1200] Batch [0/8] Loss D: 0.0008, Loss G: 7.6168\n",
            "Epoch [551/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.5828\n",
            "Epoch [552/1200] Batch [0/8] Loss D: 0.0025, Loss G: 6.6881\n",
            "Epoch [553/1200] Batch [0/8] Loss D: 0.0167, Loss G: 5.5486\n",
            "Epoch [554/1200] Batch [0/8] Loss D: 0.0099, Loss G: 5.5810\n",
            "Epoch [555/1200] Batch [0/8] Loss D: 0.0076, Loss G: 5.9578\n",
            "Epoch [556/1200] Batch [0/8] Loss D: 0.0034, Loss G: 6.8875\n",
            "Epoch [557/1200] Batch [0/8] Loss D: 0.0049, Loss G: 6.0263\n",
            "Epoch [558/1200] Batch [0/8] Loss D: 0.0100, Loss G: 4.8302\n",
            "Epoch [559/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.3484\n",
            "Epoch [560/1200] Batch [0/8] Loss D: 0.0038, Loss G: 6.7198\n",
            "Epoch [561/1200] Batch [0/8] Loss D: 0.0035, Loss G: 6.6490\n",
            "Epoch [562/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.2072\n",
            "Epoch [563/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.7820\n",
            "Epoch [564/1200] Batch [0/8] Loss D: 0.0033, Loss G: 7.1314\n",
            "Epoch [565/1200] Batch [0/8] Loss D: 0.0022, Loss G: 7.3073\n",
            "Epoch [566/1200] Batch [0/8] Loss D: 0.0024, Loss G: 7.6699\n",
            "Epoch [567/1200] Batch [0/8] Loss D: 0.0019, Loss G: 7.0500\n",
            "Epoch [568/1200] Batch [0/8] Loss D: 0.0031, Loss G: 6.6843\n",
            "Epoch [569/1200] Batch [0/8] Loss D: 0.0022, Loss G: 7.0065\n",
            "Epoch [570/1200] Batch [0/8] Loss D: 0.0052, Loss G: 6.4687\n",
            "Epoch [571/1200] Batch [0/8] Loss D: 0.0183, Loss G: 5.2891\n",
            "Epoch [572/1200] Batch [0/8] Loss D: 5.7487, Loss G: 9.8289\n",
            "Epoch [573/1200] Batch [0/8] Loss D: 0.0121, Loss G: 5.6261\n",
            "Epoch [574/1200] Batch [0/8] Loss D: 0.0160, Loss G: 5.1152\n",
            "Epoch [575/1200] Batch [0/8] Loss D: 0.0170, Loss G: 5.0781\n",
            "Epoch [576/1200] Batch [0/8] Loss D: 0.0304, Loss G: 5.1517\n",
            "Epoch [577/1200] Batch [0/8] Loss D: 0.0191, Loss G: 5.3920\n",
            "Epoch [578/1200] Batch [0/8] Loss D: 0.0202, Loss G: 7.3189\n",
            "Epoch [579/1200] Batch [0/8] Loss D: 0.0139, Loss G: 5.4596\n",
            "Epoch [580/1200] Batch [0/8] Loss D: 0.0208, Loss G: 5.2146\n",
            "Epoch [581/1200] Batch [0/8] Loss D: 0.0313, Loss G: 5.8023\n",
            "Epoch [582/1200] Batch [0/8] Loss D: 0.0140, Loss G: 5.6339\n",
            "Epoch [583/1200] Batch [0/8] Loss D: 0.0105, Loss G: 5.3329\n",
            "Epoch [584/1200] Batch [0/8] Loss D: 0.0115, Loss G: 5.5038\n",
            "Epoch [585/1200] Batch [0/8] Loss D: 0.0125, Loss G: 5.8169\n",
            "Epoch [586/1200] Batch [0/8] Loss D: 0.0111, Loss G: 5.7289\n",
            "Epoch [587/1200] Batch [0/8] Loss D: 0.0089, Loss G: 5.5452\n",
            "Epoch [588/1200] Batch [0/8] Loss D: 0.0258, Loss G: 5.3969\n",
            "Epoch [589/1200] Batch [0/8] Loss D: 0.0275, Loss G: 4.9451\n",
            "Epoch [590/1200] Batch [0/8] Loss D: 0.0201, Loss G: 5.1906\n",
            "Epoch [591/1200] Batch [0/8] Loss D: 0.0092, Loss G: 6.0274\n",
            "Epoch [592/1200] Batch [0/8] Loss D: 0.0119, Loss G: 5.6500\n",
            "Epoch [593/1200] Batch [0/8] Loss D: 0.0070, Loss G: 5.9703\n",
            "Epoch [594/1200] Batch [0/8] Loss D: 0.0068, Loss G: 5.7599\n",
            "Epoch [595/1200] Batch [0/8] Loss D: 0.0168, Loss G: 5.7160\n",
            "Epoch [596/1200] Batch [0/8] Loss D: 0.0043, Loss G: 6.6024\n",
            "Epoch [597/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.3683\n",
            "Epoch [598/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.5350\n",
            "Epoch [599/1200] Batch [0/8] Loss D: 0.0096, Loss G: 5.5744\n",
            "Epoch [600/1200] Batch [0/8] Loss D: 7.0070, Loss G: 1.2202\n",
            "Epoch [601/1200] Batch [0/8] Loss D: 0.0427, Loss G: 4.2448\n",
            "Epoch [602/1200] Batch [0/8] Loss D: 0.0256, Loss G: 4.5331\n",
            "Epoch [603/1200] Batch [0/8] Loss D: 0.0155, Loss G: 5.1396\n",
            "Epoch [604/1200] Batch [0/8] Loss D: 0.0607, Loss G: 4.4363\n",
            "Epoch [605/1200] Batch [0/8] Loss D: 0.0613, Loss G: 4.5611\n",
            "Epoch [606/1200] Batch [0/8] Loss D: 0.0345, Loss G: 4.6977\n",
            "Epoch [607/1200] Batch [0/8] Loss D: 0.0431, Loss G: 4.4019\n",
            "Epoch [608/1200] Batch [0/8] Loss D: 0.0204, Loss G: 4.6776\n",
            "Epoch [609/1200] Batch [0/8] Loss D: 0.0364, Loss G: 4.3290\n",
            "Epoch [610/1200] Batch [0/8] Loss D: 0.0273, Loss G: 4.5679\n",
            "Epoch [611/1200] Batch [0/8] Loss D: 0.0124, Loss G: 5.0015\n",
            "Epoch [612/1200] Batch [0/8] Loss D: 0.0149, Loss G: 5.3897\n",
            "Epoch [613/1200] Batch [0/8] Loss D: 0.0122, Loss G: 5.1737\n",
            "Epoch [614/1200] Batch [0/8] Loss D: 0.0155, Loss G: 5.2971\n",
            "Epoch [615/1200] Batch [0/8] Loss D: 0.0237, Loss G: 5.4241\n",
            "Epoch [616/1200] Batch [0/8] Loss D: 0.0169, Loss G: 5.1665\n",
            "Epoch [617/1200] Batch [0/8] Loss D: 0.0102, Loss G: 5.4481\n",
            "Epoch [618/1200] Batch [0/8] Loss D: 0.0091, Loss G: 5.4949\n",
            "Epoch [619/1200] Batch [0/8] Loss D: 0.0086, Loss G: 5.9730\n",
            "Epoch [620/1200] Batch [0/8] Loss D: 0.0092, Loss G: 5.9218\n",
            "Epoch [621/1200] Batch [0/8] Loss D: 0.0075, Loss G: 5.8535\n",
            "Epoch [622/1200] Batch [0/8] Loss D: 0.0063, Loss G: 5.9950\n",
            "Epoch [623/1200] Batch [0/8] Loss D: 0.0079, Loss G: 5.7270\n",
            "Epoch [624/1200] Batch [0/8] Loss D: 0.0074, Loss G: 6.1482\n",
            "Epoch [625/1200] Batch [0/8] Loss D: 0.0066, Loss G: 6.1080\n",
            "Epoch [626/1200] Batch [0/8] Loss D: 0.0058, Loss G: 6.4200\n",
            "Epoch [627/1200] Batch [0/8] Loss D: 0.0665, Loss G: 4.9213\n",
            "Epoch [628/1200] Batch [0/8] Loss D: 0.0369, Loss G: 4.5553\n",
            "Epoch [629/1200] Batch [0/8] Loss D: 0.0178, Loss G: 5.0458\n",
            "Epoch [630/1200] Batch [0/8] Loss D: 0.0296, Loss G: 4.7989\n",
            "Epoch [631/1200] Batch [0/8] Loss D: 0.0454, Loss G: 4.8092\n",
            "Epoch [632/1200] Batch [0/8] Loss D: 0.0293, Loss G: 4.6097\n",
            "Epoch [633/1200] Batch [0/8] Loss D: 0.0161, Loss G: 5.0320\n",
            "Epoch [634/1200] Batch [0/8] Loss D: 0.0121, Loss G: 5.5181\n",
            "Epoch [635/1200] Batch [0/8] Loss D: 0.0203, Loss G: 5.2122\n",
            "Epoch [636/1200] Batch [0/8] Loss D: 0.0138, Loss G: 5.7524\n",
            "Epoch [637/1200] Batch [0/8] Loss D: 0.0239, Loss G: 5.2334\n",
            "Epoch [638/1200] Batch [0/8] Loss D: 0.0210, Loss G: 4.5986\n",
            "Epoch [639/1200] Batch [0/8] Loss D: 0.0136, Loss G: 5.2271\n",
            "Epoch [640/1200] Batch [0/8] Loss D: 0.0078, Loss G: 6.0571\n",
            "Epoch [641/1200] Batch [0/8] Loss D: 0.0096, Loss G: 5.6272\n",
            "Epoch [642/1200] Batch [0/8] Loss D: 0.0057, Loss G: 6.1253\n",
            "Epoch [643/1200] Batch [0/8] Loss D: 0.0068, Loss G: 6.6331\n",
            "Epoch [644/1200] Batch [0/8] Loss D: 0.0082, Loss G: 5.8121\n",
            "Epoch [645/1200] Batch [0/8] Loss D: 0.0459, Loss G: 5.5087\n",
            "Epoch [646/1200] Batch [0/8] Loss D: 0.7441, Loss G: 5.9389\n",
            "Epoch [647/1200] Batch [0/8] Loss D: 0.0426, Loss G: 4.5663\n",
            "Epoch [648/1200] Batch [0/8] Loss D: 0.0747, Loss G: 4.0950\n",
            "Epoch [649/1200] Batch [0/8] Loss D: 0.0840, Loss G: 4.4680\n",
            "Epoch [650/1200] Batch [0/8] Loss D: 0.1744, Loss G: 4.0528\n",
            "Epoch [651/1200] Batch [0/8] Loss D: 0.0591, Loss G: 4.5082\n",
            "Epoch [652/1200] Batch [0/8] Loss D: 0.0403, Loss G: 4.1346\n",
            "Epoch [653/1200] Batch [0/8] Loss D: 0.0424, Loss G: 4.0927\n",
            "Epoch [654/1200] Batch [0/8] Loss D: 0.0410, Loss G: 3.9599\n",
            "Epoch [655/1200] Batch [0/8] Loss D: 0.0173, Loss G: 4.6002\n",
            "Epoch [656/1200] Batch [0/8] Loss D: 0.0151, Loss G: 4.8620\n",
            "Epoch [657/1200] Batch [0/8] Loss D: 0.0070, Loss G: 5.4436\n",
            "Epoch [658/1200] Batch [0/8] Loss D: 0.0101, Loss G: 5.3034\n",
            "Epoch [659/1200] Batch [0/8] Loss D: 0.0118, Loss G: 5.4391\n",
            "Epoch [660/1200] Batch [0/8] Loss D: 0.0093, Loss G: 5.6615\n",
            "Epoch [661/1200] Batch [0/8] Loss D: 0.0069, Loss G: 5.7552\n",
            "Epoch [662/1200] Batch [0/8] Loss D: 0.0159, Loss G: 5.7350\n",
            "Epoch [663/1200] Batch [0/8] Loss D: 0.0066, Loss G: 5.7255\n",
            "Epoch [664/1200] Batch [0/8] Loss D: 0.0574, Loss G: 4.7839\n",
            "Epoch [665/1200] Batch [0/8] Loss D: 0.0152, Loss G: 5.2040\n",
            "Epoch [666/1200] Batch [0/8] Loss D: 0.0122, Loss G: 5.4091\n",
            "Epoch [667/1200] Batch [0/8] Loss D: 0.0091, Loss G: 5.3926\n",
            "Epoch [668/1200] Batch [0/8] Loss D: 0.0079, Loss G: 5.5994\n",
            "Epoch [669/1200] Batch [0/8] Loss D: 0.0108, Loss G: 5.0707\n",
            "Epoch [670/1200] Batch [0/8] Loss D: 0.0051, Loss G: 6.0154\n",
            "Epoch [671/1200] Batch [0/8] Loss D: 0.0048, Loss G: 6.1048\n",
            "Epoch [672/1200] Batch [0/8] Loss D: 0.0044, Loss G: 6.2087\n",
            "Epoch [673/1200] Batch [0/8] Loss D: 0.0184, Loss G: 5.4656\n",
            "Epoch [674/1200] Batch [0/8] Loss D: 0.0852, Loss G: 4.2629\n",
            "Epoch [675/1200] Batch [0/8] Loss D: 0.0789, Loss G: 3.8712\n",
            "Epoch [676/1200] Batch [0/8] Loss D: 0.0201, Loss G: 5.1309\n",
            "Epoch [677/1200] Batch [0/8] Loss D: 0.0436, Loss G: 4.6908\n",
            "Epoch [678/1200] Batch [0/8] Loss D: 0.0097, Loss G: 5.5801\n",
            "Epoch [679/1200] Batch [0/8] Loss D: 0.0269, Loss G: 5.5168\n",
            "Epoch [680/1200] Batch [0/8] Loss D: 0.0149, Loss G: 5.3274\n",
            "Epoch [681/1200] Batch [0/8] Loss D: 0.0060, Loss G: 6.0626\n",
            "Epoch [682/1200] Batch [0/8] Loss D: 0.0071, Loss G: 5.8592\n",
            "Epoch [683/1200] Batch [0/8] Loss D: 0.0050, Loss G: 6.2382\n",
            "Epoch [684/1200] Batch [0/8] Loss D: 0.0045, Loss G: 6.3022\n",
            "Epoch [685/1200] Batch [0/8] Loss D: 0.0033, Loss G: 6.6972\n",
            "Epoch [686/1200] Batch [0/8] Loss D: 0.0034, Loss G: 6.6540\n",
            "Epoch [687/1200] Batch [0/8] Loss D: 0.0037, Loss G: 6.5961\n",
            "Epoch [688/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.4498\n",
            "Epoch [689/1200] Batch [0/8] Loss D: 0.0134, Loss G: 5.2813\n",
            "Epoch [690/1200] Batch [0/8] Loss D: 0.0107, Loss G: 5.8822\n",
            "Epoch [691/1200] Batch [0/8] Loss D: 0.0119, Loss G: 4.8420\n",
            "Epoch [692/1200] Batch [0/8] Loss D: 0.0059, Loss G: 5.9563\n",
            "Epoch [693/1200] Batch [0/8] Loss D: 0.0071, Loss G: 5.9459\n",
            "Epoch [694/1200] Batch [0/8] Loss D: 0.0091, Loss G: 5.8692\n",
            "Epoch [695/1200] Batch [0/8] Loss D: 0.0187, Loss G: 5.7661\n",
            "Epoch [696/1200] Batch [0/8] Loss D: 0.0132, Loss G: 5.4280\n",
            "Epoch [697/1200] Batch [0/8] Loss D: 0.0060, Loss G: 5.9860\n",
            "Epoch [698/1200] Batch [0/8] Loss D: 0.0088, Loss G: 5.6198\n",
            "Epoch [699/1200] Batch [0/8] Loss D: 0.0073, Loss G: 6.0688\n",
            "Epoch [700/1200] Batch [0/8] Loss D: 0.0079, Loss G: 6.0200\n",
            "Epoch [701/1200] Batch [0/8] Loss D: 0.0099, Loss G: 6.1157\n",
            "Epoch [702/1200] Batch [0/8] Loss D: 0.0071, Loss G: 6.0596\n",
            "Epoch [703/1200] Batch [0/8] Loss D: 0.0131, Loss G: 6.0814\n",
            "Epoch [704/1200] Batch [0/8] Loss D: 0.0072, Loss G: 5.9918\n",
            "Epoch [705/1200] Batch [0/8] Loss D: 0.0043, Loss G: 6.1972\n",
            "Epoch [706/1200] Batch [0/8] Loss D: 0.0043, Loss G: 6.5764\n",
            "Epoch [707/1200] Batch [0/8] Loss D: 0.0050, Loss G: 6.6041\n",
            "Epoch [708/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.8073\n",
            "Epoch [709/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.9924\n",
            "Epoch [710/1200] Batch [0/8] Loss D: 0.0044, Loss G: 6.8025\n",
            "Epoch [711/1200] Batch [0/8] Loss D: 0.0043, Loss G: 6.3258\n",
            "Epoch [712/1200] Batch [0/8] Loss D: 0.0037, Loss G: 6.9130\n",
            "Epoch [713/1200] Batch [0/8] Loss D: 0.0044, Loss G: 6.6423\n",
            "Epoch [714/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.7535\n",
            "Epoch [715/1200] Batch [0/8] Loss D: 0.0060, Loss G: 6.8244\n",
            "Epoch [716/1200] Batch [0/8] Loss D: 0.0032, Loss G: 7.0498\n",
            "Epoch [717/1200] Batch [0/8] Loss D: 0.0019, Loss G: 6.9276\n",
            "Epoch [718/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.8944\n",
            "Epoch [719/1200] Batch [0/8] Loss D: 0.0028, Loss G: 6.8307\n",
            "Epoch [720/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.9263\n",
            "Epoch [721/1200] Batch [0/8] Loss D: 0.0027, Loss G: 7.1950\n",
            "Epoch [722/1200] Batch [0/8] Loss D: 0.0037, Loss G: 7.0920\n",
            "Epoch [723/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.5201\n",
            "Epoch [724/1200] Batch [0/8] Loss D: 0.0025, Loss G: 7.1727\n",
            "Epoch [725/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.3408\n",
            "Epoch [726/1200] Batch [0/8] Loss D: 0.0035, Loss G: 7.0597\n",
            "Epoch [727/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.0460\n",
            "Epoch [728/1200] Batch [0/8] Loss D: 0.0022, Loss G: 6.9162\n",
            "Epoch [729/1200] Batch [0/8] Loss D: 0.0020, Loss G: 7.2880\n",
            "Epoch [730/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.3968\n",
            "Epoch [731/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.3618\n",
            "Epoch [732/1200] Batch [0/8] Loss D: 0.0029, Loss G: 7.2050\n",
            "Epoch [733/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.3925\n",
            "Epoch [734/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.4856\n",
            "Epoch [735/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.4227\n",
            "Epoch [736/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.4071\n",
            "Epoch [737/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.5136\n",
            "Epoch [738/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.3371\n",
            "Epoch [739/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.7658\n",
            "Epoch [740/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.4215\n",
            "Epoch [741/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.2921\n",
            "Epoch [742/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.4373\n",
            "Epoch [743/1200] Batch [0/8] Loss D: 0.0109, Loss G: 5.4965\n",
            "Epoch [744/1200] Batch [0/8] Loss D: 0.0059, Loss G: 6.1977\n",
            "Epoch [745/1200] Batch [0/8] Loss D: 0.0102, Loss G: 6.5470\n",
            "Epoch [746/1200] Batch [0/8] Loss D: 0.0188, Loss G: 5.1963\n",
            "Epoch [747/1200] Batch [0/8] Loss D: 0.0253, Loss G: 4.9826\n",
            "Epoch [748/1200] Batch [0/8] Loss D: 0.0342, Loss G: 6.0713\n",
            "Epoch [749/1200] Batch [0/8] Loss D: 0.0271, Loss G: 5.5464\n",
            "Epoch [750/1200] Batch [0/8] Loss D: 0.0132, Loss G: 5.3712\n",
            "Epoch [751/1200] Batch [0/8] Loss D: 0.0086, Loss G: 5.7250\n",
            "Epoch [752/1200] Batch [0/8] Loss D: 0.0077, Loss G: 5.6718\n",
            "Epoch [753/1200] Batch [0/8] Loss D: 0.0033, Loss G: 6.4577\n",
            "Epoch [754/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.1056\n",
            "Epoch [755/1200] Batch [0/8] Loss D: 0.0050, Loss G: 6.1400\n",
            "Epoch [756/1200] Batch [0/8] Loss D: 0.0037, Loss G: 6.4980\n",
            "Epoch [757/1200] Batch [0/8] Loss D: 0.0045, Loss G: 6.6830\n",
            "Epoch [758/1200] Batch [0/8] Loss D: 0.0020, Loss G: 6.9902\n",
            "Epoch [759/1200] Batch [0/8] Loss D: 0.0036, Loss G: 6.3039\n",
            "Epoch [760/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.8752\n",
            "Epoch [761/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.9446\n",
            "Epoch [762/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.6736\n",
            "Epoch [763/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.7551\n",
            "Epoch [764/1200] Batch [0/8] Loss D: 0.0091, Loss G: 5.8492\n",
            "Epoch [765/1200] Batch [0/8] Loss D: 0.0103, Loss G: 6.1584\n",
            "Epoch [766/1200] Batch [0/8] Loss D: 0.0044, Loss G: 6.6475\n",
            "Epoch [767/1200] Batch [0/8] Loss D: 0.0123, Loss G: 5.9737\n",
            "Epoch [768/1200] Batch [0/8] Loss D: 0.0070, Loss G: 5.9801\n",
            "Epoch [769/1200] Batch [0/8] Loss D: 0.0056, Loss G: 6.2103\n",
            "Epoch [770/1200] Batch [0/8] Loss D: 0.0084, Loss G: 6.2526\n",
            "Epoch [771/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.8434\n",
            "Epoch [772/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.1895\n",
            "Epoch [773/1200] Batch [0/8] Loss D: 0.0027, Loss G: 7.0752\n",
            "Epoch [774/1200] Batch [0/8] Loss D: 0.0048, Loss G: 5.5140\n",
            "Epoch [775/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.3783\n",
            "Epoch [776/1200] Batch [0/8] Loss D: 0.0031, Loss G: 7.0392\n",
            "Epoch [777/1200] Batch [0/8] Loss D: 0.0025, Loss G: 7.0347\n",
            "Epoch [778/1200] Batch [0/8] Loss D: 0.0227, Loss G: 6.1967\n",
            "Epoch [779/1200] Batch [0/8] Loss D: 0.3181, Loss G: 3.0461\n",
            "Epoch [780/1200] Batch [0/8] Loss D: 0.0388, Loss G: 4.3727\n",
            "Epoch [781/1200] Batch [0/8] Loss D: 0.3402, Loss G: 4.3761\n",
            "Epoch [782/1200] Batch [0/8] Loss D: 0.1738, Loss G: 3.3727\n",
            "Epoch [783/1200] Batch [0/8] Loss D: 0.0710, Loss G: 3.9944\n",
            "Epoch [784/1200] Batch [0/8] Loss D: 0.0436, Loss G: 4.1702\n",
            "Epoch [785/1200] Batch [0/8] Loss D: 0.0285, Loss G: 4.6712\n",
            "Epoch [786/1200] Batch [0/8] Loss D: 0.0209, Loss G: 4.9882\n",
            "Epoch [787/1200] Batch [0/8] Loss D: 0.0378, Loss G: 4.5180\n",
            "Epoch [788/1200] Batch [0/8] Loss D: 0.0703, Loss G: 3.9299\n",
            "Epoch [789/1200] Batch [0/8] Loss D: 0.0405, Loss G: 4.3637\n",
            "Epoch [790/1200] Batch [0/8] Loss D: 0.0268, Loss G: 4.8263\n",
            "Epoch [791/1200] Batch [0/8] Loss D: 0.0163, Loss G: 5.2555\n",
            "Epoch [792/1200] Batch [0/8] Loss D: 0.0152, Loss G: 5.1238\n",
            "Epoch [793/1200] Batch [0/8] Loss D: 0.0153, Loss G: 5.3163\n",
            "Epoch [794/1200] Batch [0/8] Loss D: 0.0175, Loss G: 4.8629\n",
            "Epoch [795/1200] Batch [0/8] Loss D: 0.0220, Loss G: 4.8527\n",
            "Epoch [796/1200] Batch [0/8] Loss D: 0.0112, Loss G: 5.2807\n",
            "Epoch [797/1200] Batch [0/8] Loss D: 0.0097, Loss G: 5.3559\n",
            "Epoch [798/1200] Batch [0/8] Loss D: 0.0070, Loss G: 5.6723\n",
            "Epoch [799/1200] Batch [0/8] Loss D: 0.0066, Loss G: 5.7908\n",
            "Epoch [800/1200] Batch [0/8] Loss D: 0.0062, Loss G: 5.7660\n",
            "Epoch [801/1200] Batch [0/8] Loss D: 0.0064, Loss G: 5.8567\n",
            "Epoch [802/1200] Batch [0/8] Loss D: 0.0069, Loss G: 5.7669\n",
            "Epoch [803/1200] Batch [0/8] Loss D: 0.0058, Loss G: 6.0622\n",
            "Epoch [804/1200] Batch [0/8] Loss D: 0.0084, Loss G: 5.9504\n",
            "Epoch [805/1200] Batch [0/8] Loss D: 0.0054, Loss G: 5.9470\n",
            "Epoch [806/1200] Batch [0/8] Loss D: 0.0046, Loss G: 6.1200\n",
            "Epoch [807/1200] Batch [0/8] Loss D: 0.0058, Loss G: 5.8820\n",
            "Epoch [808/1200] Batch [0/8] Loss D: 0.0065, Loss G: 6.3267\n",
            "Epoch [809/1200] Batch [0/8] Loss D: 0.0045, Loss G: 6.4034\n",
            "Epoch [810/1200] Batch [0/8] Loss D: 0.0055, Loss G: 6.1287\n",
            "Epoch [811/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.4038\n",
            "Epoch [812/1200] Batch [0/8] Loss D: 0.0063, Loss G: 6.1679\n",
            "Epoch [813/1200] Batch [0/8] Loss D: 0.0065, Loss G: 6.1796\n",
            "Epoch [814/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.5930\n",
            "Epoch [815/1200] Batch [0/8] Loss D: 0.0061, Loss G: 5.9737\n",
            "Epoch [816/1200] Batch [0/8] Loss D: 0.0323, Loss G: 4.7933\n",
            "Epoch [817/1200] Batch [0/8] Loss D: 0.0446, Loss G: 4.0702\n",
            "Epoch [818/1200] Batch [0/8] Loss D: 0.0323, Loss G: 4.9569\n",
            "Epoch [819/1200] Batch [0/8] Loss D: 0.0221, Loss G: 4.2955\n",
            "Epoch [820/1200] Batch [0/8] Loss D: 0.0108, Loss G: 5.2320\n",
            "Epoch [821/1200] Batch [0/8] Loss D: 0.0153, Loss G: 5.1355\n",
            "Epoch [822/1200] Batch [0/8] Loss D: 0.0082, Loss G: 5.5102\n",
            "Epoch [823/1200] Batch [0/8] Loss D: 0.0078, Loss G: 5.7687\n",
            "Epoch [824/1200] Batch [0/8] Loss D: 0.0060, Loss G: 5.7029\n",
            "Epoch [825/1200] Batch [0/8] Loss D: 0.0071, Loss G: 5.5627\n",
            "Epoch [826/1200] Batch [0/8] Loss D: 0.0069, Loss G: 5.6514\n",
            "Epoch [827/1200] Batch [0/8] Loss D: 0.0067, Loss G: 5.7183\n",
            "Epoch [828/1200] Batch [0/8] Loss D: 0.0075, Loss G: 5.9453\n",
            "Epoch [829/1200] Batch [0/8] Loss D: 0.0065, Loss G: 5.8366\n",
            "Epoch [830/1200] Batch [0/8] Loss D: 0.0056, Loss G: 5.9656\n",
            "Epoch [831/1200] Batch [0/8] Loss D: 0.0354, Loss G: 4.3493\n",
            "Epoch [832/1200] Batch [0/8] Loss D: 0.0260, Loss G: 4.7018\n",
            "Epoch [833/1200] Batch [0/8] Loss D: 0.0259, Loss G: 4.5706\n",
            "Epoch [834/1200] Batch [0/8] Loss D: 0.0136, Loss G: 5.1898\n",
            "Epoch [835/1200] Batch [0/8] Loss D: 0.0142, Loss G: 5.3150\n",
            "Epoch [836/1200] Batch [0/8] Loss D: 0.0137, Loss G: 5.0481\n",
            "Epoch [837/1200] Batch [0/8] Loss D: 0.0127, Loss G: 5.1244\n",
            "Epoch [838/1200] Batch [0/8] Loss D: 0.0116, Loss G: 5.2809\n",
            "Epoch [839/1200] Batch [0/8] Loss D: 0.0102, Loss G: 5.8211\n",
            "Epoch [840/1200] Batch [0/8] Loss D: 0.0081, Loss G: 5.7193\n",
            "Epoch [841/1200] Batch [0/8] Loss D: 0.0087, Loss G: 5.5883\n",
            "Epoch [842/1200] Batch [0/8] Loss D: 0.0250, Loss G: 4.6701\n",
            "Epoch [843/1200] Batch [0/8] Loss D: 0.0746, Loss G: 5.2253\n",
            "Epoch [844/1200] Batch [0/8] Loss D: 0.0420, Loss G: 4.8779\n",
            "Epoch [845/1200] Batch [0/8] Loss D: 0.0397, Loss G: 4.0248\n",
            "Epoch [846/1200] Batch [0/8] Loss D: 0.0317, Loss G: 4.4672\n",
            "Epoch [847/1200] Batch [0/8] Loss D: 0.0360, Loss G: 3.9429\n",
            "Epoch [848/1200] Batch [0/8] Loss D: 0.0170, Loss G: 4.6920\n",
            "Epoch [849/1200] Batch [0/8] Loss D: 0.0140, Loss G: 5.1654\n",
            "Epoch [850/1200] Batch [0/8] Loss D: 0.0102, Loss G: 5.1551\n",
            "Epoch [851/1200] Batch [0/8] Loss D: 0.0093, Loss G: 5.4905\n",
            "Epoch [852/1200] Batch [0/8] Loss D: 0.0134, Loss G: 5.2440\n",
            "Epoch [853/1200] Batch [0/8] Loss D: 0.0111, Loss G: 5.3232\n",
            "Epoch [854/1200] Batch [0/8] Loss D: 0.0088, Loss G: 5.6487\n",
            "Epoch [855/1200] Batch [0/8] Loss D: 0.0079, Loss G: 5.6770\n",
            "Epoch [856/1200] Batch [0/8] Loss D: 0.0069, Loss G: 5.8283\n",
            "Epoch [857/1200] Batch [0/8] Loss D: 0.0092, Loss G: 5.5675\n",
            "Epoch [858/1200] Batch [0/8] Loss D: 0.0970, Loss G: 4.5944\n",
            "Epoch [859/1200] Batch [0/8] Loss D: 0.0781, Loss G: 5.0057\n",
            "Epoch [860/1200] Batch [0/8] Loss D: 0.0265, Loss G: 4.4521\n",
            "Epoch [861/1200] Batch [0/8] Loss D: 0.0385, Loss G: 4.5926\n",
            "Epoch [862/1200] Batch [0/8] Loss D: 0.0238, Loss G: 4.6039\n",
            "Epoch [863/1200] Batch [0/8] Loss D: 0.0186, Loss G: 4.7253\n",
            "Epoch [864/1200] Batch [0/8] Loss D: 0.0169, Loss G: 4.7899\n",
            "Epoch [865/1200] Batch [0/8] Loss D: 0.0150, Loss G: 5.3603\n",
            "Epoch [866/1200] Batch [0/8] Loss D: 0.0124, Loss G: 5.4975\n",
            "Epoch [867/1200] Batch [0/8] Loss D: 0.0287, Loss G: 4.8537\n",
            "Epoch [868/1200] Batch [0/8] Loss D: 0.0355, Loss G: 4.1014\n",
            "Epoch [869/1200] Batch [0/8] Loss D: 0.0166, Loss G: 4.9513\n",
            "Epoch [870/1200] Batch [0/8] Loss D: 0.0861, Loss G: 4.7146\n",
            "Epoch [871/1200] Batch [0/8] Loss D: 0.0666, Loss G: 3.8692\n",
            "Epoch [872/1200] Batch [0/8] Loss D: 0.0423, Loss G: 4.5183\n",
            "Epoch [873/1200] Batch [0/8] Loss D: 0.0236, Loss G: 4.7260\n",
            "Epoch [874/1200] Batch [0/8] Loss D: 0.0699, Loss G: 3.2747\n",
            "Epoch [875/1200] Batch [0/8] Loss D: 0.0421, Loss G: 4.1013\n",
            "Epoch [876/1200] Batch [0/8] Loss D: 0.0152, Loss G: 5.4892\n",
            "Epoch [877/1200] Batch [0/8] Loss D: 0.0138, Loss G: 4.8053\n",
            "Epoch [878/1200] Batch [0/8] Loss D: 0.0134, Loss G: 5.2836\n",
            "Epoch [879/1200] Batch [0/8] Loss D: 0.0110, Loss G: 5.4422\n",
            "Epoch [880/1200] Batch [0/8] Loss D: 0.0121, Loss G: 5.2479\n",
            "Epoch [881/1200] Batch [0/8] Loss D: 0.0106, Loss G: 5.4561\n",
            "Epoch [882/1200] Batch [0/8] Loss D: 0.0122, Loss G: 5.0269\n",
            "Epoch [883/1200] Batch [0/8] Loss D: 0.0083, Loss G: 5.7555\n",
            "Epoch [884/1200] Batch [0/8] Loss D: 0.0109, Loss G: 5.5051\n",
            "Epoch [885/1200] Batch [0/8] Loss D: 0.0071, Loss G: 5.5456\n",
            "Epoch [886/1200] Batch [0/8] Loss D: 0.0098, Loss G: 5.2789\n",
            "Epoch [887/1200] Batch [0/8] Loss D: 0.0049, Loss G: 5.9895\n",
            "Epoch [888/1200] Batch [0/8] Loss D: 0.0048, Loss G: 6.0507\n",
            "Epoch [889/1200] Batch [0/8] Loss D: 0.0052, Loss G: 6.0825\n",
            "Epoch [890/1200] Batch [0/8] Loss D: 0.0090, Loss G: 6.0728\n",
            "Epoch [891/1200] Batch [0/8] Loss D: 0.0094, Loss G: 5.1664\n",
            "Epoch [892/1200] Batch [0/8] Loss D: 0.0171, Loss G: 5.7397\n",
            "Epoch [893/1200] Batch [0/8] Loss D: 0.0071, Loss G: 6.3124\n",
            "Epoch [894/1200] Batch [0/8] Loss D: 0.0064, Loss G: 6.0667\n",
            "Epoch [895/1200] Batch [0/8] Loss D: 0.0109, Loss G: 5.2638\n",
            "Epoch [896/1200] Batch [0/8] Loss D: 0.0162, Loss G: 5.6234\n",
            "Epoch [897/1200] Batch [0/8] Loss D: 0.1191, Loss G: 7.1402\n",
            "Epoch [898/1200] Batch [0/8] Loss D: 0.0114, Loss G: 5.2739\n",
            "Epoch [899/1200] Batch [0/8] Loss D: 0.0061, Loss G: 6.1716\n",
            "Epoch [900/1200] Batch [0/8] Loss D: 0.0065, Loss G: 6.2953\n",
            "Epoch [901/1200] Batch [0/8] Loss D: 0.0073, Loss G: 6.0548\n",
            "Epoch [902/1200] Batch [0/8] Loss D: 0.0033, Loss G: 6.4669\n",
            "Epoch [903/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.6961\n",
            "Epoch [904/1200] Batch [0/8] Loss D: 0.0031, Loss G: 6.6976\n",
            "Epoch [905/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.6834\n",
            "Epoch [906/1200] Batch [0/8] Loss D: 0.0025, Loss G: 6.9510\n",
            "Epoch [907/1200] Batch [0/8] Loss D: 0.0031, Loss G: 6.5379\n",
            "Epoch [908/1200] Batch [0/8] Loss D: 0.0026, Loss G: 6.8772\n",
            "Epoch [909/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.6141\n",
            "Epoch [910/1200] Batch [0/8] Loss D: 0.0035, Loss G: 7.5159\n",
            "Epoch [911/1200] Batch [0/8] Loss D: 0.0020, Loss G: 7.0660\n",
            "Epoch [912/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.7163\n",
            "Epoch [913/1200] Batch [0/8] Loss D: 0.0020, Loss G: 6.9859\n",
            "Epoch [914/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.2501\n",
            "Epoch [915/1200] Batch [0/8] Loss D: 0.0018, Loss G: 6.8758\n",
            "Epoch [916/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.2787\n",
            "Epoch [917/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.6453\n",
            "Epoch [918/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.0367\n",
            "Epoch [919/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.7018\n",
            "Epoch [920/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.1423\n",
            "Epoch [921/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.6220\n",
            "Epoch [922/1200] Batch [0/8] Loss D: 0.0020, Loss G: 6.9564\n",
            "Epoch [923/1200] Batch [0/8] Loss D: 0.0106, Loss G: 6.6605\n",
            "Epoch [924/1200] Batch [0/8] Loss D: 0.0188, Loss G: 5.2107\n",
            "Epoch [925/1200] Batch [0/8] Loss D: 0.0136, Loss G: 5.5749\n",
            "Epoch [926/1200] Batch [0/8] Loss D: 0.0087, Loss G: 5.5728\n",
            "Epoch [927/1200] Batch [0/8] Loss D: 0.0065, Loss G: 5.9180\n",
            "Epoch [928/1200] Batch [0/8] Loss D: 0.0059, Loss G: 5.6955\n",
            "Epoch [929/1200] Batch [0/8] Loss D: 0.0047, Loss G: 6.6235\n",
            "Epoch [930/1200] Batch [0/8] Loss D: 0.0052, Loss G: 6.3975\n",
            "Epoch [931/1200] Batch [0/8] Loss D: 0.0087, Loss G: 6.6995\n",
            "Epoch [932/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.9139\n",
            "Epoch [933/1200] Batch [0/8] Loss D: 0.0019, Loss G: 7.0917\n",
            "Epoch [934/1200] Batch [0/8] Loss D: 0.0031, Loss G: 6.3962\n",
            "Epoch [935/1200] Batch [0/8] Loss D: 0.0023, Loss G: 7.0538\n",
            "Epoch [936/1200] Batch [0/8] Loss D: 0.0038, Loss G: 6.8926\n",
            "Epoch [937/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.3173\n",
            "Epoch [938/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.0683\n",
            "Epoch [939/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.2977\n",
            "Epoch [940/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.4857\n",
            "Epoch [941/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.0834\n",
            "Epoch [942/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.2121\n",
            "Epoch [943/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.8075\n",
            "Epoch [944/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.7223\n",
            "Epoch [945/1200] Batch [0/8] Loss D: 0.0026, Loss G: 7.4731\n",
            "Epoch [946/1200] Batch [0/8] Loss D: 0.0019, Loss G: 7.5097\n",
            "Epoch [947/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.5575\n",
            "Epoch [948/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.6523\n",
            "Epoch [949/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.6658\n",
            "Epoch [950/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.7454\n",
            "Epoch [951/1200] Batch [0/8] Loss D: 0.0022, Loss G: 7.5987\n",
            "Epoch [952/1200] Batch [0/8] Loss D: 0.0028, Loss G: 7.2733\n",
            "Epoch [953/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.6891\n",
            "Epoch [954/1200] Batch [0/8] Loss D: 0.0022, Loss G: 6.9092\n",
            "Epoch [955/1200] Batch [0/8] Loss D: 0.0058, Loss G: 5.4065\n",
            "Epoch [956/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.7345\n",
            "Epoch [957/1200] Batch [0/8] Loss D: 0.0020, Loss G: 6.6787\n",
            "Epoch [958/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.0551\n",
            "Epoch [959/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.3856\n",
            "Epoch [960/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.3483\n",
            "Epoch [961/1200] Batch [0/8] Loss D: 0.0024, Loss G: 7.0451\n",
            "Epoch [962/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.2661\n",
            "Epoch [963/1200] Batch [0/8] Loss D: 0.0019, Loss G: 7.3271\n",
            "Epoch [964/1200] Batch [0/8] Loss D: 0.0031, Loss G: 7.1334\n",
            "Epoch [965/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.1886\n",
            "Epoch [966/1200] Batch [0/8] Loss D: 0.0017, Loss G: 6.9712\n",
            "Epoch [967/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.2311\n",
            "Epoch [968/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.8881\n",
            "Epoch [969/1200] Batch [0/8] Loss D: 0.0013, Loss G: 7.8429\n",
            "Epoch [970/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.4442\n",
            "Epoch [971/1200] Batch [0/8] Loss D: 0.0026, Loss G: 7.2654\n",
            "Epoch [972/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.3392\n",
            "Epoch [973/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.9677\n",
            "Epoch [974/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.8429\n",
            "Epoch [975/1200] Batch [0/8] Loss D: 0.0019, Loss G: 7.5110\n",
            "Epoch [976/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.3869\n",
            "Epoch [977/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.8438\n",
            "Epoch [978/1200] Batch [0/8] Loss D: 0.0024, Loss G: 7.6584\n",
            "Epoch [979/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.4790\n",
            "Epoch [980/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.6159\n",
            "Epoch [981/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.8375\n",
            "Epoch [982/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.9539\n",
            "Epoch [983/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.8089\n",
            "Epoch [984/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.9230\n",
            "Epoch [985/1200] Batch [0/8] Loss D: 0.0009, Loss G: 8.3102\n",
            "Epoch [986/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.7189\n",
            "Epoch [987/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.8046\n",
            "Epoch [988/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.8495\n",
            "Epoch [989/1200] Batch [0/8] Loss D: 0.0307, Loss G: 7.4332\n",
            "Epoch [990/1200] Batch [0/8] Loss D: 0.1360, Loss G: 5.0203\n",
            "Epoch [991/1200] Batch [0/8] Loss D: 0.0303, Loss G: 4.9618\n",
            "Epoch [992/1200] Batch [0/8] Loss D: 0.1727, Loss G: 2.9781\n",
            "Epoch [993/1200] Batch [0/8] Loss D: 0.0580, Loss G: 4.1641\n",
            "Epoch [994/1200] Batch [0/8] Loss D: 0.0461, Loss G: 3.9986\n",
            "Epoch [995/1200] Batch [0/8] Loss D: 0.0503, Loss G: 3.9812\n",
            "Epoch [996/1200] Batch [0/8] Loss D: 0.0489, Loss G: 4.5256\n",
            "Epoch [997/1200] Batch [0/8] Loss D: 0.0490, Loss G: 3.9569\n",
            "Epoch [998/1200] Batch [0/8] Loss D: 0.2121, Loss G: 3.4858\n",
            "Epoch [999/1200] Batch [0/8] Loss D: 0.1216, Loss G: 4.2733\n",
            "Epoch [1000/1200] Batch [0/8] Loss D: 0.0469, Loss G: 4.1542\n",
            "Epoch [1001/1200] Batch [0/8] Loss D: 0.2120, Loss G: 4.1971\n",
            "Epoch [1002/1200] Batch [0/8] Loss D: 0.0695, Loss G: 3.5441\n",
            "Epoch [1003/1200] Batch [0/8] Loss D: 0.0992, Loss G: 3.0926\n",
            "Epoch [1004/1200] Batch [0/8] Loss D: 0.1030, Loss G: 3.5097\n",
            "Epoch [1005/1200] Batch [0/8] Loss D: 0.0602, Loss G: 3.7207\n",
            "Epoch [1006/1200] Batch [0/8] Loss D: 0.0509, Loss G: 3.8939\n",
            "Epoch [1007/1200] Batch [0/8] Loss D: 0.0660, Loss G: 3.7881\n",
            "Epoch [1008/1200] Batch [0/8] Loss D: 0.0486, Loss G: 4.1610\n",
            "Epoch [1009/1200] Batch [0/8] Loss D: 0.0312, Loss G: 4.1037\n",
            "Epoch [1010/1200] Batch [0/8] Loss D: 0.0580, Loss G: 3.8126\n",
            "Epoch [1011/1200] Batch [0/8] Loss D: 0.0192, Loss G: 4.6037\n",
            "Epoch [1012/1200] Batch [0/8] Loss D: 0.0253, Loss G: 4.4646\n",
            "Epoch [1013/1200] Batch [0/8] Loss D: 0.0284, Loss G: 4.5225\n",
            "Epoch [1014/1200] Batch [0/8] Loss D: 0.0249, Loss G: 4.2935\n",
            "Epoch [1015/1200] Batch [0/8] Loss D: 0.0206, Loss G: 4.5000\n",
            "Epoch [1016/1200] Batch [0/8] Loss D: 0.0102, Loss G: 5.1226\n",
            "Epoch [1017/1200] Batch [0/8] Loss D: 0.0237, Loss G: 4.8079\n",
            "Epoch [1018/1200] Batch [0/8] Loss D: 0.0145, Loss G: 5.2211\n",
            "Epoch [1019/1200] Batch [0/8] Loss D: 0.0089, Loss G: 5.2579\n",
            "Epoch [1020/1200] Batch [0/8] Loss D: 0.0089, Loss G: 5.2799\n",
            "Epoch [1021/1200] Batch [0/8] Loss D: 0.0135, Loss G: 4.9677\n",
            "Epoch [1022/1200] Batch [0/8] Loss D: 0.0107, Loss G: 5.1931\n",
            "Epoch [1023/1200] Batch [0/8] Loss D: 0.0098, Loss G: 5.4807\n",
            "Epoch [1024/1200] Batch [0/8] Loss D: 0.0106, Loss G: 5.0891\n",
            "Epoch [1025/1200] Batch [0/8] Loss D: 0.0140, Loss G: 4.8077\n",
            "Epoch [1026/1200] Batch [0/8] Loss D: 0.0115, Loss G: 5.2692\n",
            "Epoch [1027/1200] Batch [0/8] Loss D: 0.0079, Loss G: 5.5531\n",
            "Epoch [1028/1200] Batch [0/8] Loss D: 0.0095, Loss G: 5.5794\n",
            "Epoch [1029/1200] Batch [0/8] Loss D: 0.0110, Loss G: 5.2286\n",
            "Epoch [1030/1200] Batch [0/8] Loss D: 0.0106, Loss G: 5.4179\n",
            "Epoch [1031/1200] Batch [0/8] Loss D: 0.0074, Loss G: 5.4400\n",
            "Epoch [1032/1200] Batch [0/8] Loss D: 0.0083, Loss G: 5.4435\n",
            "Epoch [1033/1200] Batch [0/8] Loss D: 0.0053, Loss G: 5.8946\n",
            "Epoch [1034/1200] Batch [0/8] Loss D: 0.0079, Loss G: 5.9521\n",
            "Epoch [1035/1200] Batch [0/8] Loss D: 0.0054, Loss G: 6.0667\n",
            "Epoch [1036/1200] Batch [0/8] Loss D: 0.0113, Loss G: 5.6035\n",
            "Epoch [1037/1200] Batch [0/8] Loss D: 0.0085, Loss G: 5.6921\n",
            "Epoch [1038/1200] Batch [0/8] Loss D: 0.0045, Loss G: 6.0957\n",
            "Epoch [1039/1200] Batch [0/8] Loss D: 0.0052, Loss G: 5.5378\n",
            "Epoch [1040/1200] Batch [0/8] Loss D: 0.0055, Loss G: 5.7328\n",
            "Epoch [1041/1200] Batch [0/8] Loss D: 0.0057, Loss G: 5.5861\n",
            "Epoch [1042/1200] Batch [0/8] Loss D: 0.0069, Loss G: 5.4334\n",
            "Epoch [1043/1200] Batch [0/8] Loss D: 0.0088, Loss G: 6.0681\n",
            "Epoch [1044/1200] Batch [0/8] Loss D: 0.0045, Loss G: 6.5663\n",
            "Epoch [1045/1200] Batch [0/8] Loss D: 0.0064, Loss G: 5.4549\n",
            "Epoch [1046/1200] Batch [0/8] Loss D: 0.0059, Loss G: 5.8939\n",
            "Epoch [1047/1200] Batch [0/8] Loss D: 0.0078, Loss G: 5.7440\n",
            "Epoch [1048/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.3032\n",
            "Epoch [1049/1200] Batch [0/8] Loss D: 0.0054, Loss G: 5.8972\n",
            "Epoch [1050/1200] Batch [0/8] Loss D: 0.0028, Loss G: 6.4987\n",
            "Epoch [1051/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.3960\n",
            "Epoch [1052/1200] Batch [0/8] Loss D: 0.0057, Loss G: 5.6532\n",
            "Epoch [1053/1200] Batch [0/8] Loss D: 0.0048, Loss G: 6.1959\n",
            "Epoch [1054/1200] Batch [0/8] Loss D: 0.0082, Loss G: 6.2837\n",
            "Epoch [1055/1200] Batch [0/8] Loss D: 0.0026, Loss G: 6.3610\n",
            "Epoch [1056/1200] Batch [0/8] Loss D: 0.0052, Loss G: 6.2026\n",
            "Epoch [1057/1200] Batch [0/8] Loss D: 0.0039, Loss G: 6.5900\n",
            "Epoch [1058/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.4889\n",
            "Epoch [1059/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.1750\n",
            "Epoch [1060/1200] Batch [0/8] Loss D: 0.0051, Loss G: 5.9877\n",
            "Epoch [1061/1200] Batch [0/8] Loss D: 0.0051, Loss G: 6.4851\n",
            "Epoch [1062/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.4077\n",
            "Epoch [1063/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.2412\n",
            "Epoch [1064/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.4726\n",
            "Epoch [1065/1200] Batch [0/8] Loss D: 0.0033, Loss G: 6.5402\n",
            "Epoch [1066/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.2522\n",
            "Epoch [1067/1200] Batch [0/8] Loss D: 0.0040, Loss G: 6.6072\n",
            "Epoch [1068/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.4195\n",
            "Epoch [1069/1200] Batch [0/8] Loss D: 0.0024, Loss G: 6.5552\n",
            "Epoch [1070/1200] Batch [0/8] Loss D: 0.0024, Loss G: 6.6959\n",
            "Epoch [1071/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.8132\n",
            "Epoch [1072/1200] Batch [0/8] Loss D: 0.0041, Loss G: 6.7341\n",
            "Epoch [1073/1200] Batch [0/8] Loss D: 0.0026, Loss G: 6.9161\n",
            "Epoch [1074/1200] Batch [0/8] Loss D: 0.0042, Loss G: 6.5100\n",
            "Epoch [1075/1200] Batch [0/8] Loss D: 0.0035, Loss G: 6.6875\n",
            "Epoch [1076/1200] Batch [0/8] Loss D: 0.0023, Loss G: 7.0058\n",
            "Epoch [1077/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.9192\n",
            "Epoch [1078/1200] Batch [0/8] Loss D: 0.0015, Loss G: 6.9818\n",
            "Epoch [1079/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.7534\n",
            "Epoch [1080/1200] Batch [0/8] Loss D: 0.0017, Loss G: 6.9532\n",
            "Epoch [1081/1200] Batch [0/8] Loss D: 0.0038, Loss G: 6.6431\n",
            "Epoch [1082/1200] Batch [0/8] Loss D: 0.0037, Loss G: 6.8678\n",
            "Epoch [1083/1200] Batch [0/8] Loss D: 0.0018, Loss G: 6.7256\n",
            "Epoch [1084/1200] Batch [0/8] Loss D: 0.0023, Loss G: 6.7982\n",
            "Epoch [1085/1200] Batch [0/8] Loss D: 0.0019, Loss G: 6.8738\n",
            "Epoch [1086/1200] Batch [0/8] Loss D: 0.0017, Loss G: 6.7876\n",
            "Epoch [1087/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.0754\n",
            "Epoch [1088/1200] Batch [0/8] Loss D: 0.0025, Loss G: 7.1914\n",
            "Epoch [1089/1200] Batch [0/8] Loss D: 0.0018, Loss G: 7.0649\n",
            "Epoch [1090/1200] Batch [0/8] Loss D: 0.0018, Loss G: 6.9889\n",
            "Epoch [1091/1200] Batch [0/8] Loss D: 0.0035, Loss G: 6.8826\n",
            "Epoch [1092/1200] Batch [0/8] Loss D: 0.0025, Loss G: 7.3135\n",
            "Epoch [1093/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.0553\n",
            "Epoch [1094/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.0862\n",
            "Epoch [1095/1200] Batch [0/8] Loss D: 0.0045, Loss G: 6.1416\n",
            "Epoch [1096/1200] Batch [0/8] Loss D: 0.0017, Loss G: 6.8086\n",
            "Epoch [1097/1200] Batch [0/8] Loss D: 0.0033, Loss G: 6.4797\n",
            "Epoch [1098/1200] Batch [0/8] Loss D: 0.0028, Loss G: 6.6770\n",
            "Epoch [1099/1200] Batch [0/8] Loss D: 0.0025, Loss G: 6.7927\n",
            "Epoch [1100/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.0859\n",
            "Epoch [1101/1200] Batch [0/8] Loss D: 0.0028, Loss G: 6.8202\n",
            "Epoch [1102/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.9986\n",
            "Epoch [1103/1200] Batch [0/8] Loss D: 0.0022, Loss G: 6.7069\n",
            "Epoch [1104/1200] Batch [0/8] Loss D: 0.0019, Loss G: 6.7668\n",
            "Epoch [1105/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.6119\n",
            "Epoch [1106/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.2117\n",
            "Epoch [1107/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.0951\n",
            "Epoch [1108/1200] Batch [0/8] Loss D: 0.0017, Loss G: 6.7036\n",
            "Epoch [1109/1200] Batch [0/8] Loss D: 0.0028, Loss G: 7.0231\n",
            "Epoch [1110/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.1549\n",
            "Epoch [1111/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.1420\n",
            "Epoch [1112/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.2006\n",
            "Epoch [1113/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.3261\n",
            "Epoch [1114/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.3421\n",
            "Epoch [1115/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.3252\n",
            "Epoch [1116/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.1421\n",
            "Epoch [1117/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.0739\n",
            "Epoch [1118/1200] Batch [0/8] Loss D: 0.0011, Loss G: 6.9503\n",
            "Epoch [1119/1200] Batch [0/8] Loss D: 0.0026, Loss G: 6.9202\n",
            "Epoch [1120/1200] Batch [0/8] Loss D: 0.0016, Loss G: 6.9052\n",
            "Epoch [1121/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.4223\n",
            "Epoch [1122/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.2645\n",
            "Epoch [1123/1200] Batch [0/8] Loss D: 0.0033, Loss G: 6.6025\n",
            "Epoch [1124/1200] Batch [0/8] Loss D: 0.0038, Loss G: 6.3302\n",
            "Epoch [1125/1200] Batch [0/8] Loss D: 0.0066, Loss G: 5.8896\n",
            "Epoch [1126/1200] Batch [0/8] Loss D: 0.0066, Loss G: 5.6612\n",
            "Epoch [1127/1200] Batch [0/8] Loss D: 0.0069, Loss G: 6.0577\n",
            "Epoch [1128/1200] Batch [0/8] Loss D: 0.0050, Loss G: 6.4524\n",
            "Epoch [1129/1200] Batch [0/8] Loss D: 0.0103, Loss G: 6.2873\n",
            "Epoch [1130/1200] Batch [0/8] Loss D: 0.0026, Loss G: 6.7569\n",
            "Epoch [1131/1200] Batch [0/8] Loss D: 0.0091, Loss G: 5.6930\n",
            "Epoch [1132/1200] Batch [0/8] Loss D: 0.0038, Loss G: 6.6528\n",
            "Epoch [1133/1200] Batch [0/8] Loss D: 0.0037, Loss G: 6.7734\n",
            "Epoch [1134/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.8957\n",
            "Epoch [1135/1200] Batch [0/8] Loss D: 0.0038, Loss G: 6.2180\n",
            "Epoch [1136/1200] Batch [0/8] Loss D: 0.0046, Loss G: 6.4588\n",
            "Epoch [1137/1200] Batch [0/8] Loss D: 0.0047, Loss G: 6.6279\n",
            "Epoch [1138/1200] Batch [0/8] Loss D: 0.0020, Loss G: 6.9261\n",
            "Epoch [1139/1200] Batch [0/8] Loss D: 0.0029, Loss G: 6.2548\n",
            "Epoch [1140/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.3139\n",
            "Epoch [1141/1200] Batch [0/8] Loss D: 0.0025, Loss G: 6.6952\n",
            "Epoch [1142/1200] Batch [0/8] Loss D: 0.0023, Loss G: 7.4546\n",
            "Epoch [1143/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.3624\n",
            "Epoch [1144/1200] Batch [0/8] Loss D: 0.0030, Loss G: 6.8194\n",
            "Epoch [1145/1200] Batch [0/8] Loss D: 0.0023, Loss G: 7.0470\n",
            "Epoch [1146/1200] Batch [0/8] Loss D: 0.0021, Loss G: 6.6300\n",
            "Epoch [1147/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.9191\n",
            "Epoch [1148/1200] Batch [0/8] Loss D: 0.0024, Loss G: 7.2733\n",
            "Epoch [1149/1200] Batch [0/8] Loss D: 0.0023, Loss G: 7.5508\n",
            "Epoch [1150/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.4503\n",
            "Epoch [1151/1200] Batch [0/8] Loss D: 0.0027, Loss G: 6.6752\n",
            "Epoch [1152/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.4267\n",
            "Epoch [1153/1200] Batch [0/8] Loss D: 0.0025, Loss G: 6.8266\n",
            "Epoch [1154/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.2478\n",
            "Epoch [1155/1200] Batch [0/8] Loss D: 0.0018, Loss G: 6.8875\n",
            "Epoch [1156/1200] Batch [0/8] Loss D: 0.0032, Loss G: 6.5426\n",
            "Epoch [1157/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.6612\n",
            "Epoch [1158/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.6225\n",
            "Epoch [1159/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.2569\n",
            "Epoch [1160/1200] Batch [0/8] Loss D: 0.0023, Loss G: 7.6337\n",
            "Epoch [1161/1200] Batch [0/8] Loss D: 0.0055, Loss G: 7.2721\n",
            "Epoch [1162/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.4026\n",
            "Epoch [1163/1200] Batch [0/8] Loss D: 0.0021, Loss G: 7.0204\n",
            "Epoch [1164/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.4603\n",
            "Epoch [1165/1200] Batch [0/8] Loss D: 0.0028, Loss G: 7.3483\n",
            "Epoch [1166/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.5931\n",
            "Epoch [1167/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.9717\n",
            "Epoch [1168/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.2394\n",
            "Epoch [1169/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.9128\n",
            "Epoch [1170/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.6547\n",
            "Epoch [1171/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.4982\n",
            "Epoch [1172/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.6566\n",
            "Epoch [1173/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.4932\n",
            "Epoch [1174/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.6498\n",
            "Epoch [1175/1200] Batch [0/8] Loss D: 0.0025, Loss G: 7.4257\n",
            "Epoch [1176/1200] Batch [0/8] Loss D: 0.0007, Loss G: 7.8477\n",
            "Epoch [1177/1200] Batch [0/8] Loss D: 0.0011, Loss G: 7.7295\n",
            "Epoch [1178/1200] Batch [0/8] Loss D: 0.0014, Loss G: 6.3795\n",
            "Epoch [1179/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.4204\n",
            "Epoch [1180/1200] Batch [0/8] Loss D: 0.0014, Loss G: 7.5060\n",
            "Epoch [1181/1200] Batch [0/8] Loss D: 0.0017, Loss G: 7.6148\n",
            "Epoch [1182/1200] Batch [0/8] Loss D: 0.0033, Loss G: 7.0944\n",
            "Epoch [1183/1200] Batch [0/8] Loss D: 0.0012, Loss G: 7.5006\n",
            "Epoch [1184/1200] Batch [0/8] Loss D: 0.0007, Loss G: 7.9067\n",
            "Epoch [1185/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.4909\n",
            "Epoch [1186/1200] Batch [0/8] Loss D: 0.0015, Loss G: 7.8903\n",
            "Epoch [1187/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.7234\n",
            "Epoch [1188/1200] Batch [0/8] Loss D: 0.0016, Loss G: 7.8725\n",
            "Epoch [1189/1200] Batch [0/8] Loss D: 0.0008, Loss G: 8.1366\n",
            "Epoch [1190/1200] Batch [0/8] Loss D: 0.0007, Loss G: 7.8710\n",
            "Epoch [1191/1200] Batch [0/8] Loss D: 0.0009, Loss G: 7.4718\n",
            "Epoch [1192/1200] Batch [0/8] Loss D: 0.0008, Loss G: 7.9267\n",
            "Epoch [1193/1200] Batch [0/8] Loss D: 0.0006, Loss G: 8.1835\n",
            "Epoch [1194/1200] Batch [0/8] Loss D: 0.0010, Loss G: 7.6008\n",
            "Epoch [1195/1200] Batch [0/8] Loss D: 0.0007, Loss G: 8.0643\n",
            "Epoch [1196/1200] Batch [0/8] Loss D: 0.0006, Loss G: 8.2696\n",
            "Epoch [1197/1200] Batch [0/8] Loss D: 0.0007, Loss G: 7.9220\n",
            "Epoch [1198/1200] Batch [0/8] Loss D: 0.0007, Loss G: 7.9964\n",
            "Epoch [1199/1200] Batch [0/8] Loss D: 0.0022, Loss G: 8.1483\n",
            "Epoch [1200/1200] Batch [0/8] Loss D: 0.0008, Loss G: 7.8849\n",
            "Training completed and models saved.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Function to generate a single 32x32 RGB image with the letter \"T\" and a random horizontal line\n",
        "def generate_image(horizontal_line_position):\n",
        "    # Initialize a 32x32 image with white background\n",
        "    image = np.ones((32, 32, 3), dtype=np.uint8) * 255\n",
        "\n",
        "    # Define the color for the \"T\" shape (black)\n",
        "    color = np.zeros((3,), dtype=np.uint8)  # Black color\n",
        "\n",
        "    # Draw the vertical line of \"T\"\n",
        "    image[:, 12:14] = color  # Vertical line of \"T\" (2 pixels wide)\n",
        "\n",
        "    # Draw the horizontal line of \"T\" at the given position with increased thickness\n",
        "    if horizontal_line_position >= 0:\n",
        "        image[horizontal_line_position:horizontal_line_position + 2, 6:26] = color  # Thicker horizontal line of \"T\" (20 pixels wide, 2 pixels thick)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Function to create a dataset\n",
        "def create_dataset(num_images, save_dir):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Randomly choose the horizontal line position within image bounds\n",
        "        horizontal_line_position = np.random.randint(0, 32)  # Random position from 0 to 31\n",
        "        image = generate_image(horizontal_line_position)\n",
        "        img = Image.fromarray(image, 'RGB')\n",
        "        img.save(os.path.join(save_dir, f'image_{i}.png'))\n",
        "\n",
        "# Number of images to generate\n",
        "num_images = 1000  # Adjust as needed\n",
        "save_dir = 'T_dataset'\n",
        "\n",
        "# Generate and save the dataset\n",
        "create_dataset(num_images, save_dir)\n",
        "\n",
        "print(f'Dataset generated and saved in {save_dir}')\n",
        "\n",
        "# PyTorch part\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define the custom dataset\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, 0  # We don't need labels for GANs, so we return a dummy label\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, 256, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Define the discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1)\n",
        "\n",
        "# Hyperparameters\n",
        "nz = 120  # Size of the latent vector (input to the generator)\n",
        "lr = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 1200\n",
        "\n",
        "# Data preprocessing and loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = CustomImageDataset(image_dir='T_dataset', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "netG = Generator(nz).to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, _) in enumerate(dataloader):\n",
        "        # Update discriminator\n",
        "        netD.zero_grad()\n",
        "        real_data = data.to(device)\n",
        "        batch_size = real_data.size(0)\n",
        "        labels = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
        "\n",
        "        output = netD(real_data)\n",
        "        errD_real = criterion(output, labels)\n",
        "        errD_real.backward()\n",
        "\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake_data = netG(noise)\n",
        "        labels.fill_(0)\n",
        "\n",
        "        output = netD(fake_data.detach())\n",
        "        errD_fake = criterion(output, labels)\n",
        "        errD_fake.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Update generator\n",
        "        netG.zero_grad()\n",
        "        labels.fill_(1)\n",
        "\n",
        "        output = netD(fake_data)\n",
        "        errG = criterion(output, labels)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Print training stats\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
        "                  f\"Loss D: {errD_real.item() + errD_fake.item():.4f}, Loss G: {errG.item():.4f}\")\n",
        "\n",
        "# Save the models\n",
        "torch.save(netG.state_dict(), 'generator.pth')\n",
        "torch.save(netD.state_dict(), 'discriminator.pth')\n",
        "\n",
        "print(\"Training completed and models saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the Generator class\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, 256, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Set up the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "nz = 120  # Size of the latent vector (input to the generator)\n",
        "num_images = 100  # Number of images to generate\n",
        "image_size = 128  # Size of the output image\n",
        "\n",
        "# Initialize the generator\n",
        "netG = Generator(nz).to(device)\n",
        "\n",
        "# Load the saved model\n",
        "netG.load_state_dict(torch.load('generator.pth', map_location=device))\n",
        "netG.eval()  # Set to evaluation mode\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = 'high_quality_generated_images'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Define image transformation for upscaling\n",
        "upscale_transform = transforms.Compose([\n",
        "    transforms.Resize(image_size, interpolation=Image.BICUBIC),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Convert grayscale to RGB\n",
        "])\n",
        "\n",
        "# Generate images\n",
        "with torch.no_grad():\n",
        "    for i in range(num_images):\n",
        "        # Generate random noise\n",
        "        noise = torch.randn(1, nz, 1, 1, device=device)\n",
        "\n",
        "        # Generate fake image\n",
        "        fake_image = netG(noise)\n",
        "\n",
        "        # Rescale images to be between 0 and 1\n",
        "        fake_image = (fake_image + 1) / 2.0\n",
        "\n",
        "        # Upscale and convert to RGB\n",
        "        fake_image = upscale_transform(fake_image.squeeze(0))\n",
        "\n",
        "        # Save the image\n",
        "        save_image(fake_image, f'{output_dir}/high_quality_generated_image_{i+1}.png',\n",
        "                   nrow=1, padding=0, normalize=False)\n",
        "\n",
        "print(f\"Generated {num_images} high-quality images in the '{output_dir}' directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgHjZQZ5gytX",
        "outputId": "8f2b5c39-566e-4f97-ca3d-c46886d7a698"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100 high-quality images in the 'high_quality_generated_images' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNzQ9sPhjf18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}